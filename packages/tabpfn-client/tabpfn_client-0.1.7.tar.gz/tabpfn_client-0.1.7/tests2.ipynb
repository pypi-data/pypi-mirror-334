{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.3-py3-none-macosx_12_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /Users/leo/mambaforge/envs/skrub/lib/python3.10/site-packages (from xgboost) (1.11.1)\n",
      "Requirement already satisfied: numpy in /Users/leo/mambaforge/envs/skrub/lib/python3.10/site-packages (from xgboost) (1.25.2)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TableVectorizer\n",
    "from skrub.datasets import fetch_medical_charge\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, TargetEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tabpfn_client import TabPFNClassifier, TabPFNRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#df = pd.read_parquet(\"clear_corpus.parquet\")\n",
    "df = pd.read_csv(\"medical_charge_small.csv\")\n",
    "X, y = df.drop(columns=[\"target\"]), df[\"target\"]\n",
    "#y = (y > np.median(y)).astype(int)\n",
    "#X, y = df.X, df.y\n",
    "\n",
    "#X, y = X[:1500], y[:1500]\n",
    "\n",
    "# # convert to classification\n",
    "# y = (y > np.median(y)).astype(int)\n",
    "\n",
    "task = \"regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_text_embeddings import PredictorWithSkrub, PredictorWithTextEmbeddings, FixedSizeSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_vectorizer_tabpfn = TableVectorizer(high_cardinality=TargetEncoder(target_type=\"continuous\", random_state=34),\n",
    "                                            low_cardinality=OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                                                            unknown_value=np.nan))\n",
    "table_vectorizer_xgboost = TableVectorizer(high_cardinality=TargetEncoder(target_type=\"continuous\"),\n",
    "                                            low_cardinality=OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                                                            unknown_value=np.nan))\n",
    "table_vectorizer_random_forest = TableVectorizer(high_cardinality=TargetEncoder(target_type=\"auto\"),\n",
    "                                                    low_cardinality=OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                                                                    unknown_value=np.nan))\n",
    "table_vectorizer_linear_regression = TableVectorizer(high_cardinality=TargetEncoder(target_type=\"auto\"))\n",
    "\n",
    "# Create pipelines that combine preprocessing and models\n",
    "pipelines = {\n",
    "    'TabPFN2': Pipeline([\n",
    "        ('vectorizer', table_vectorizer_tabpfn),\n",
    "        ('model', TabPFNClassifier() if task == \"classification\" \n",
    "                    else TabPFNRegressor())\n",
    "    ]),\n",
    "    # \"TabPFN2_with_text_embeddings\": #Pipeline([\n",
    "    #     #('vectorizer', table_vectorizer_tabpfn),\n",
    "    #     PredictorWithTextEmbeddings(api_key=\"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\",\n",
    "    #                                 base_predictor=TabPFNClassifier() if task == \"classification\" \n",
    "    #                                 #else TabPFNRegressor())\n",
    "    # ]),\n",
    "    'Random Forest': Pipeline([\n",
    "        ('vectorizer', table_vectorizer_random_forest),\n",
    "        ('model', RandomForestClassifier(random_state=42) if task == \"classification\"\n",
    "                    else RandomForestRegressor(random_state=42))\n",
    "    ]),\n",
    "    'XGBoost': Pipeline([\n",
    "        ('vectorizer', table_vectorizer_xgboost), \n",
    "        ('model', XGBClassifier(random_state=42) if task == \"classification\"\n",
    "                    else XGBRegressor(random_state=42))\n",
    "    ]),\n",
    "    'Logistic Regression' if task == \"classification\" else 'Linear Regression': Pipeline([\n",
    "        ('vectorizer', table_vectorizer_linear_regression),\n",
    "        ('model', LogisticRegression(random_state=42) if task == \"classification\"\n",
    "                    else LinearRegression())\n",
    "    ])\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "#cv_splitter = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_splitter = FixedSizeSplit(n_splits=7, n_train=1000, n_test=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6440373824588305"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(pipelines[\"XGBoost\"], X, y, cv=cv_splitter, scoring=\"r2\").mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7739244923292748"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(pipelines[\"TabPFN2\"], X, y, cv=cv_splitter, scoring=\"r2\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 =  PredictorWithTextEmbeddings(api_key=\"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\",\n",
    "                                base_predictor=TabPFNClassifier() if task == \"classification\" \n",
    "                                else TabPFNRegressor())\n",
    "model = PredictorWithSkrub(base_predictor=model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot clone object '<utils_text_embeddings.PredictorWithSkrub object at 0x283d809d0>' (type <class 'utils_text_embeddings.PredictorWithSkrub'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/skrub/lib/python3.10/site-packages/joblib/parallel.py:862\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 862\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ready_batches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m queue\u001b[38;5;241m.\u001b[39mEmpty:\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;66;03m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;66;03m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;66;03m# accordingly to distribute evenly the last items between all\u001b[39;00m\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# workers.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/skrub/lib/python3.10/queue.py:168\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 168\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv_splitter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroc_auc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/mambaforge/envs/skrub/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:562\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    560\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 562\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/skrub/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/skrub/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:309\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 309\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/skrub/lib/python3.10/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/skrub/lib/python3.10/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/mambaforge/envs/skrub/lib/python3.10/site-packages/joblib/parallel.py:873\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    870\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_effective_n_jobs\n\u001b[1;32m    871\u001b[0m big_batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m n_jobs\n\u001b[0;32m--> 873\u001b[0m islice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbig_batch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(islice) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/skrub/lib/python3.10/site-packages/sklearn/utils/parallel.py:61\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Capture the thread-local scikit-learn configuration at the time\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Parallel.__call__ is issued since the tasks can be dispatched\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# in a different thread depending on the backend and on the value of\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# pre_dispatch and n_jobs.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m---> 61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/mambaforge/envs/skrub/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:311\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m    309\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    310\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m--> 311\u001b[0m         \u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    312\u001b[0m         X,\n\u001b[1;32m    313\u001b[0m         y,\n\u001b[1;32m    314\u001b[0m         scorers,\n\u001b[1;32m    315\u001b[0m         train,\n\u001b[1;32m    316\u001b[0m         test,\n\u001b[1;32m    317\u001b[0m         verbose,\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    319\u001b[0m         fit_params,\n\u001b[1;32m    320\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[1;32m    321\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    322\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[1;32m    323\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[1;32m    324\u001b[0m     )\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[1;32m    326\u001b[0m )\n\u001b[1;32m    328\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/skrub/lib/python3.10/site-packages/sklearn/base.py:76\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_clone__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misclass(estimator):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m estimator\u001b[38;5;241m.\u001b[39m__sklearn_clone__()\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_clone_parametrized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/skrub/lib/python3.10/site-packages/sklearn/base.py:97\u001b[0m, in \u001b[0;36m_clone_parametrized\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     92\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should provide an instance of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscikit-learn estimator instead of a class.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m             )\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit does not seem to be a scikit-learn \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator as it does not implement a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mrepr\u001b[39m(estimator), \u001b[38;5;28mtype\u001b[39m(estimator))\n\u001b[1;32m    102\u001b[0m             )\n\u001b[1;32m    104\u001b[0m klass \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[1;32m    105\u001b[0m new_object_params \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mget_params(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot clone object '<utils_text_embeddings.PredictorWithSkrub object at 0x283d809d0>' (type <class 'utils_text_embeddings.PredictorWithSkrub'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method."
     ]
    }
   ],
   "source": [
    "cross_val_score(model, X, y, cv=cv_splitter, scoring=\"roc_auc\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old_score 0.8800563236047108\n",
      "old_score 0.8630152329749103\n",
      "old_score 0.8916842711734203\n",
      "old_score 0.8599945529406109\n",
      "old_score 0.8523557692307692\n",
      "old_score 0.8854801462569761\n",
      "old_score 0.8813405797101449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8800563236047108,\n",
       " 0.8630152329749103,\n",
       " 0.8916842711734203,\n",
       " 0.8599945529406109,\n",
       " 0.8523557692307692,\n",
       " 0.8854801462569761,\n",
       " 0.8813405797101449]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "old_list = []\n",
    "for split in cv_splitter.split(X, y):\n",
    "    X_train_split = X.iloc[split[0]]\n",
    "    y_train_split = y.iloc[split[0]]\n",
    "    X_test_split = X.iloc[split[1]]\n",
    "    model_old = pipelines[\"TabPFN2\"]\n",
    "    model_old.fit(X_train_split, y_train_split)\n",
    "    y_pred = model_old.predict_proba(X_test_split)[:, 1]\n",
    "    old_score = roc_auc_score(y.iloc[split[1]], y_pred)\n",
    "    old_list.append(old_score)\n",
    "    print(\"old_score\", old_score)\n",
    "\n",
    "old_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7942588325652842,\n",
       " 0.7723054275473631,\n",
       " 0.818241469816273,\n",
       " 0.7614748714333777,\n",
       " 0.7605769230769229,\n",
       " 0.8184456988902431,\n",
       " 0.8078739648033126]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Author', 'name', 'Pub_Year']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.text_embedder.ignored_text_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_score 0.7493779909940925\n",
      "old_score 0.7613423116081036\n",
      "new_score 0.7729615601443744\n",
      "old_score 0.7777887006135938\n",
      "new_score 0.8033936439771733\n",
      "old_score 0.8096929823190757\n",
      "new_score 0.7811588618367495\n",
      "old_score 0.7834562179161612\n",
      "new_score 0.7481265582498935\n",
      "old_score 0.7543438738894334\n",
      "new_score 0.7238323002300628\n",
      "old_score 0.7304864320277922\n",
      "new_score 0.8349600553130726\n",
      "old_score 0.8355963059735774\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, r2_score\n",
    "old_list = []\n",
    "new_list = []\n",
    "model1 =  PredictorWithTextEmbeddings(api_key=\"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\",\n",
    "                                base_predictor=TabPFNClassifier() if task == \"classification\" \n",
    "                                else TabPFNRegressor())\n",
    "model = PredictorWithSkrub(base_predictor=model1)\n",
    "for split in cv_splitter.split(X, y):\n",
    "    # transform the data\n",
    "    X_train_split = X.iloc[split[0]]\n",
    "    y_train_split = y.iloc[split[0]]\n",
    "    #X_test_split = table_vectorizer_tabpfn.transform(X.iloc[split[1]])\n",
    "    X_test_split = X.iloc[split[1]]\n",
    "    y_test_split = y.iloc[split[1]]\n",
    "    await model.fit(X_train_split, y_train_split)\n",
    "    #y_pred = await model.predict_proba(X_test_split)\n",
    "    y_pred = await model.predict(X_test_split)\n",
    "    #y_pred = y_pred[:, 1]\n",
    "    #new_score = roc_auc_score(y_test_split, y_pred)\n",
    "    new_score = r2_score(y_test_split, y_pred)\n",
    "    new_list.append(new_score)\n",
    "    model_old = pipelines[\"TabPFN2\"]\n",
    "    model_old.fit(X_train_split, y_train_split)\n",
    "    #y_pred = model_old.predict_proba(X_test_split)[:, 1]\n",
    "    y_pred = model_old.predict(X_test_split)\n",
    "    #old_score = roc_auc_score(y_test_split, y_pred)\n",
    "    old_score = r2_score(y_test_split, y_pred)\n",
    "    old_list.append(old_score)\n",
    "    print(\"new_score\", new_score)\n",
    "    print(\"old_score\", old_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_score 0.749746786442413\n",
      "old_score 0.7613423116081036\n",
      "new_score 0.7729124517770994\n",
      "old_score 0.7777887006135938\n",
      "new_score 0.8037766623715148\n",
      "old_score 0.8096929823190757\n",
      "new_score 0.7811703390434003\n",
      "old_score 0.7834562179161612\n",
      "new_score 0.7477482678228928\n",
      "old_score 0.7543438738894334\n",
      "new_score 0.7232958549601941\n",
      "old_score 0.7304864320277922\n",
      "new_score 0.8341665889815265\n",
      "old_score 0.8355963059735774\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "old_list = []\n",
    "new_list = []\n",
    "for split in cv_splitter.split(X, y):\n",
    "    model1 =  PredictorWithTextEmbeddings(api_key=\"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\",\n",
    "                                base_predictor=TabPFNClassifier() if task == \"classification\" \n",
    "                                else TabPFNRegressor())\n",
    "    model = PredictorWithSkrub(base_predictor=model1)\n",
    "    # transform the data\n",
    "    X_train_split = X.iloc[split[0]]\n",
    "    y_train_split = y.iloc[split[0]]\n",
    "    #X_test_split = table_vectorizer_tabpfn.transform(X.iloc[split[1]])\n",
    "    X_test_split = X.iloc[split[1]]\n",
    "    y_test_split = y.iloc[split[1]]\n",
    "    await model.fit(X_train_split, y_train_split)\n",
    "    #y_pred = await model.predict_proba(X_test_split)\n",
    "    y_pred = await model.predict(X_test_split)\n",
    "    #y_pred = y_pred[:, 1]\n",
    "    #new_score = roc_auc_score(y_test_split, y_pred)\n",
    "    new_score = r2_score(y_test_split, y_pred)    \n",
    "    new_list.append(new_score)\n",
    "    model_old = pipelines[\"TabPFN2\"]\n",
    "    model_old.fit(X_train_split, y_train_split)\n",
    "    #y_pred = model_old.predict_proba(X_test_split)[:, 1]\n",
    "    y_pred = model_old.predict(X_test_split)\n",
    "    #old_score = roc_auc_score(y_test_split, y_pred)\n",
    "    old_score = r2_score(y_test_split, y_pred)\n",
    "    old_list.append(old_score)\n",
    "    print(\"new_score\", new_score)\n",
    "    print(\"old_score\", old_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7732595644855774 0.7789581177639625\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(new_list), np.mean(old_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8883281573498963 0.8813405797101449\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(new_score), np.mean(old_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.50834725,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.71439873, 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.76872501, 1.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.50834725, 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.37722751, 0.        , 1.        , 0.        , 0.50834725,\n",
       "        0.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.50834725, 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.63422314,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 0.63422314, 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.37722751, 1.        , 0.23696135, 0.        ,\n",
       "        0.50834725, 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 0.50501002, 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.50834725, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.50834725, 1.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.50834725, 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 0.29351266, 1.        , 0.50834725, 1.        ,\n",
       "        0.71439873, 0.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.71439873, 1.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.42017654, 0.50834725, 0.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.50834725,\n",
       "        1.        , 0.        , 1.        , 0.50834725, 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        0.37722751, 0.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 0.50834725, 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.50834725, 0.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.50834725, 1.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.50834725, 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.71439873, 1.        , 1.        , 0.63422314, 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.37722751, 1.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.71439873,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 0.63422314, 0.        ,\n",
       "        1.        , 0.        , 1.        , 0.37722751, 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.50501002, 1.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 0.63422314, 1.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.63422314, 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.37722751, 1.        ,\n",
       "        1.        , 1.        , 0.50834725, 1.        , 1.        ,\n",
       "        0.37722751, 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.61637453, 1.        , 0.17557045,\n",
       "        0.24410903]),\n",
       " array([1.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 0.50501002]),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.37722751,\n",
       "        0.42017654, 0.        , 0.23696135, 0.        , 0.        ,\n",
       "        0.50834725, 0.        , 0.        , 0.50834725, 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.80714346, 0.63422314,\n",
       "        0.37722751, 0.29351266, 0.        , 0.        , 1.        ,\n",
       "        0.77518754, 1.        , 0.90691754, 1.        , 0.72021502,\n",
       "        0.31076104, 0.80714346, 0.50075942, 0.07044722, 0.17319785,\n",
       "        0.16817449, 0.56571637, 0.        , 0.63422314, 0.80714346,\n",
       "        0.19710354, 0.        , 0.        , 0.23696135, 0.61448696,\n",
       "        0.29351266, 0.58789407, 1.        , 0.69447261, 1.        ,\n",
       "        0.7306701 , 0.35060109, 0.23696135, 0.58789407, 0.5035791 ,\n",
       "        0.71439873, 0.39076955, 0.80714346, 0.4075365 , 0.45242837,\n",
       "        0.27362543, 0.50278396, 0.63558485, 0.63558485, 0.34658256,\n",
       "        0.28152958, 0.16297442, 0.42480469, 0.50119332, 0.36173615,\n",
       "        0.50147406, 0.75655173, 0.57630769, 0.50092816, 0.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.50834725, 0.37722751, 0.50834725, 0.69447261, 1.        ,\n",
       "        1.        , 1.        , 0.76872501, 0.80714346, 0.87904185,\n",
       "        0.68199203, 0.59721981, 0.82173159, 0.82371024, 0.74185955,\n",
       "        0.50847138, 1.        ])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_predictor.text_embedder.target_encoder.encodings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Google_WC': PassThrough(),\n",
       " 'Sentence_Count': PassThrough(),\n",
       " 'Paragraphs': PassThrough(),\n",
       " 'Flesch-Reading-Ease': PassThrough(),\n",
       " 'Flesch-Kincaid-Grade-Level': PassThrough(),\n",
       " 'Automated_Readability_Index': PassThrough(),\n",
       " 'SMOG_Readability': PassThrough(),\n",
       " 'New_Dale-Chall_Readability_Formula': PassThrough(),\n",
       " 'CAREC': PassThrough(),\n",
       " 'CAREC_M': PassThrough(),\n",
       " 'CML2RI': PassThrough(),\n",
       " 'Categ': OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=nan),\n",
       " 'Lexile_Band': OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=nan),\n",
       " 'Location': OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=nan),\n",
       " 'MPAA_Max': OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=nan),\n",
       " 'Author': TargetEncoder(target_type='continuous'),\n",
       " 'name': TargetEncoder(target_type='continuous'),\n",
       " 'Pub_Year': TargetEncoder(target_type='continuous')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"vectorizer\"].transformers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1414    1\n",
       "456     1\n",
       "195     1\n",
       "31      0\n",
       "1257    0\n",
       "       ..\n",
       "2110    0\n",
       "3004    1\n",
       "3723    1\n",
       "2718    1\n",
       "3825    0\n",
       "Name: target, Length: 1000, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_split[X_train_split[\"Author\"] == \"Sarat Talluri Rao\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6128712237583205\n"
     ]
    }
   ],
   "source": [
    "    y_pred = await model.predict(X_test_split)\n",
    "    print(roc_auc_score(y.iloc[split[1]], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>name</th>\n",
       "      <th>Pub_Year</th>\n",
       "      <th>Categ</th>\n",
       "      <th>Lexile_Band</th>\n",
       "      <th>Location</th>\n",
       "      <th>MPAA_Max</th>\n",
       "      <th>Google_WC</th>\n",
       "      <th>Sentence_Count</th>\n",
       "      <th>Paragraphs</th>\n",
       "      <th>Flesch-Reading-Ease</th>\n",
       "      <th>Flesch-Kincaid-Grade-Level</th>\n",
       "      <th>Automated_Readability_Index</th>\n",
       "      <th>SMOG_Readability</th>\n",
       "      <th>New_Dale-Chall_Readability_Formula</th>\n",
       "      <th>CAREC</th>\n",
       "      <th>CAREC_M</th>\n",
       "      <th>CML2RI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>Shelby Ostergaard</td>\n",
       "      <td>How the Internet Came to Be</td>\n",
       "      <td>2017</td>\n",
       "      <td>Info</td>\n",
       "      <td>1100</td>\n",
       "      <td>mid</td>\n",
       "      <td>G</td>\n",
       "      <td>183</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>61.32</td>\n",
       "      <td>8.26</td>\n",
       "      <td>8.91</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.52</td>\n",
       "      <td>0.27610</td>\n",
       "      <td>0.26069</td>\n",
       "      <td>18.804362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>Daniel Montelongo-Jauregui, Ahmed S. Sultan, T...</td>\n",
       "      <td>COVID-19: Fighting a Virus Gone Viral</td>\n",
       "      <td>2020</td>\n",
       "      <td>Info</td>\n",
       "      <td>900</td>\n",
       "      <td>mid</td>\n",
       "      <td>PG</td>\n",
       "      <td>193</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>55.14</td>\n",
       "      <td>10.09</td>\n",
       "      <td>9.84</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.67</td>\n",
       "      <td>0.28897</td>\n",
       "      <td>0.26659</td>\n",
       "      <td>17.268422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>NEPHI ANDERSON</td>\n",
       "      <td>A Young Folks' History OF THE CHURCH OF JESUS ...</td>\n",
       "      <td>1916</td>\n",
       "      <td>Lit</td>\n",
       "      <td>1300</td>\n",
       "      <td>start</td>\n",
       "      <td>G</td>\n",
       "      <td>142</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>70.17</td>\n",
       "      <td>10.53</td>\n",
       "      <td>11.67</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.94</td>\n",
       "      <td>0.09571</td>\n",
       "      <td>0.12771</td>\n",
       "      <td>18.603055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Edward Sylvester Ellis</td>\n",
       "      <td>THE WRITING FOUND IN A BOTTLE</td>\n",
       "      <td>1903</td>\n",
       "      <td>Lit</td>\n",
       "      <td>1100</td>\n",
       "      <td>mid</td>\n",
       "      <td>PG</td>\n",
       "      <td>140</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>57.10</td>\n",
       "      <td>11.17</td>\n",
       "      <td>12.55</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.48</td>\n",
       "      <td>0.09479</td>\n",
       "      <td>0.13946</td>\n",
       "      <td>9.028936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>Sarat Talluri Rao</td>\n",
       "      <td>Talking in Twos</td>\n",
       "      <td>2018</td>\n",
       "      <td>Info</td>\n",
       "      <td>700</td>\n",
       "      <td>mid</td>\n",
       "      <td>G</td>\n",
       "      <td>189</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>78.38</td>\n",
       "      <td>4.74</td>\n",
       "      <td>3.15</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.94</td>\n",
       "      <td>0.24146</td>\n",
       "      <td>0.23669</td>\n",
       "      <td>26.025942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>EMIL VERHAEREN</td>\n",
       "      <td>Address to King Albert of Belgium</td>\n",
       "      <td>1915</td>\n",
       "      <td>Info</td>\n",
       "      <td>900</td>\n",
       "      <td>start</td>\n",
       "      <td>G</td>\n",
       "      <td>175</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>79.61</td>\n",
       "      <td>6.15</td>\n",
       "      <td>6.82</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.38</td>\n",
       "      <td>0.11013</td>\n",
       "      <td>0.13165</td>\n",
       "      <td>17.352955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3004</th>\n",
       "      <td>M. C. W.</td>\n",
       "      <td>BOUNCER</td>\n",
       "      <td>1881</td>\n",
       "      <td>Lit</td>\n",
       "      <td>700</td>\n",
       "      <td>mid</td>\n",
       "      <td>G</td>\n",
       "      <td>157</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>90.30</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.28</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.10</td>\n",
       "      <td>0.01320</td>\n",
       "      <td>0.02188</td>\n",
       "      <td>21.313707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3723</th>\n",
       "      <td>Henry Cabot Lodge and Theodore Roosevelt</td>\n",
       "      <td>Hero Tales from American History</td>\n",
       "      <td>1895</td>\n",
       "      <td>Info</td>\n",
       "      <td>1300</td>\n",
       "      <td>mid</td>\n",
       "      <td>G</td>\n",
       "      <td>171</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>63.30</td>\n",
       "      <td>10.50</td>\n",
       "      <td>11.31</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.71</td>\n",
       "      <td>0.11499</td>\n",
       "      <td>0.13713</td>\n",
       "      <td>13.676537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2718</th>\n",
       "      <td>?</td>\n",
       "      <td>PHYSICS WITHOUT APPARATUS</td>\n",
       "      <td>1883</td>\n",
       "      <td>Info</td>\n",
       "      <td>1300</td>\n",
       "      <td>start</td>\n",
       "      <td>PG</td>\n",
       "      <td>140</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>54.54</td>\n",
       "      <td>12.61</td>\n",
       "      <td>13.59</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.39</td>\n",
       "      <td>0.21861</td>\n",
       "      <td>0.27265</td>\n",
       "      <td>18.429061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3825</th>\n",
       "      <td>A. T. Quiller-Couch</td>\n",
       "      <td>The Man Behind the Curtain</td>\n",
       "      <td>1904</td>\n",
       "      <td>Lit</td>\n",
       "      <td>1100</td>\n",
       "      <td>mid</td>\n",
       "      <td>G</td>\n",
       "      <td>184</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>52.61</td>\n",
       "      <td>15.21</td>\n",
       "      <td>17.41</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.28</td>\n",
       "      <td>0.20233</td>\n",
       "      <td>0.21128</td>\n",
       "      <td>7.769616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Author  \\\n",
       "1414                                  Shelby Ostergaard   \n",
       "456   Daniel Montelongo-Jauregui, Ahmed S. Sultan, T...   \n",
       "195                                      NEPHI ANDERSON   \n",
       "31                               Edward Sylvester Ellis   \n",
       "1257                                  Sarat Talluri Rao   \n",
       "...                                                 ...   \n",
       "2110                                     EMIL VERHAEREN   \n",
       "3004                                           M. C. W.   \n",
       "3723           Henry Cabot Lodge and Theodore Roosevelt   \n",
       "2718                                                  ?   \n",
       "3825                                A. T. Quiller-Couch   \n",
       "\n",
       "                                                   name Pub_Year Categ  \\\n",
       "1414                        How the Internet Came to Be     2017  Info   \n",
       "456               COVID-19: Fighting a Virus Gone Viral     2020  Info   \n",
       "195   A Young Folks' History OF THE CHURCH OF JESUS ...     1916   Lit   \n",
       "31                        THE WRITING FOUND IN A BOTTLE     1903   Lit   \n",
       "1257                                    Talking in Twos     2018  Info   \n",
       "...                                                 ...      ...   ...   \n",
       "2110                  Address to King Albert of Belgium     1915  Info   \n",
       "3004                                            BOUNCER     1881   Lit   \n",
       "3723                   Hero Tales from American History     1895  Info   \n",
       "2718                          PHYSICS WITHOUT APPARATUS     1883  Info   \n",
       "3825                         The Man Behind the Curtain     1904   Lit   \n",
       "\n",
       "     Lexile_Band Location MPAA_Max  Google_WC  Sentence_Count  Paragraphs  \\\n",
       "1414        1100      mid        G        183              13           3   \n",
       "456          900      mid       PG        193              11           2   \n",
       "195         1300    start        G        142               5           1   \n",
       "31          1100      mid       PG        140               6           2   \n",
       "1257         700      mid        G        189              20           6   \n",
       "...          ...      ...      ...        ...             ...         ...   \n",
       "2110         900    start        G        175              11           3   \n",
       "3004         700      mid        G        157              14           4   \n",
       "3723        1300      mid        G        171               7           1   \n",
       "2718        1300    start       PG        140               5           1   \n",
       "3825        1100      mid        G        184               5           1   \n",
       "\n",
       "      Flesch-Reading-Ease  Flesch-Kincaid-Grade-Level  \\\n",
       "1414                61.32                        8.26   \n",
       "456                 55.14                       10.09   \n",
       "195                 70.17                       10.53   \n",
       "31                  57.10                       11.17   \n",
       "1257                78.38                        4.74   \n",
       "...                   ...                         ...   \n",
       "2110                79.61                        6.15   \n",
       "3004                90.30                        3.78   \n",
       "3723                63.30                       10.50   \n",
       "2718                54.54                       12.61   \n",
       "3825                52.61                       15.21   \n",
       "\n",
       "      Automated_Readability_Index  SMOG_Readability  \\\n",
       "1414                         8.91              11.0   \n",
       "456                          9.84              12.0   \n",
       "195                         11.67               8.0   \n",
       "31                          12.55              11.0   \n",
       "1257                         3.15               9.0   \n",
       "...                           ...               ...   \n",
       "2110                         6.82               9.0   \n",
       "3004                         3.28               7.0   \n",
       "3723                        11.31              12.0   \n",
       "2718                        13.59              12.0   \n",
       "3825                        17.41              12.0   \n",
       "\n",
       "      New_Dale-Chall_Readability_Formula    CAREC  CAREC_M     CML2RI  \n",
       "1414                                9.52  0.27610  0.26069  18.804362  \n",
       "456                                 9.67  0.28897  0.26659  17.268422  \n",
       "195                                 6.94  0.09571  0.12771  18.603055  \n",
       "31                                  7.48  0.09479  0.13946   9.028936  \n",
       "1257                                7.94  0.24146  0.23669  26.025942  \n",
       "...                                  ...      ...      ...        ...  \n",
       "2110                                7.38  0.11013  0.13165  17.352955  \n",
       "3004                                6.10  0.01320  0.02188  21.313707  \n",
       "3723                                7.71  0.11499  0.13713  13.676537  \n",
       "2718                                7.39  0.21861  0.27265  18.429061  \n",
       "3825                                8.28  0.20233  0.21128   7.769616  \n",
       "\n",
       "[1000 rows x 18 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "4719    1\n",
       "4720    1\n",
       "4721    1\n",
       "4722    1\n",
       "4723    1\n",
       "Name: target, Length: 4724, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Author', 'name', 'Pub_Year'], dtype=object)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_predictor.text_embedder.target_encoder.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'target_type_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fy/__8z8cpn6gs04465sq9g1nq80000gn/T/ipykernel_47581/3235134159.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_train_split\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_type_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/mambaforge/envs/skrub/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6200\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[1;32m   6203\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'target_type_'"
     ]
    }
   ],
   "source": [
    "y_train_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.50834725,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.71439873, 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.76872501, 1.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.50834725, 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.37722751, 0.        , 1.        , 0.        , 0.50834725,\n",
       "        0.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.50834725, 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.63422314,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 0.63422314, 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.37722751, 1.        , 0.23696135, 0.        ,\n",
       "        0.50834725, 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 0.50501002, 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.50834725, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.50834725, 1.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.50834725, 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 0.29351266, 1.        , 0.50834725, 1.        ,\n",
       "        0.71439873, 0.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.71439873, 1.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.42017654, 0.50834725, 0.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.50834725,\n",
       "        1.        , 0.        , 1.        , 0.50834725, 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        0.37722751, 0.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 0.50834725, 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.50834725, 0.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.50834725, 1.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.50834725, 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.71439873, 1.        , 1.        , 0.63422314, 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.37722751, 1.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.71439873,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 0.63422314, 0.        ,\n",
       "        1.        , 0.        , 1.        , 0.37722751, 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.50501002, 1.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 0.63422314, 1.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.63422314, 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.37722751, 1.        ,\n",
       "        1.        , 1.        , 0.50834725, 1.        , 1.        ,\n",
       "        0.37722751, 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.61637453, 1.        , 0.17557045,\n",
       "        0.24410903]),\n",
       " array([1.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 1.        , 0.50501002]),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.37722751,\n",
       "        0.42017654, 0.        , 0.23696135, 0.        , 0.        ,\n",
       "        0.50834725, 0.        , 0.        , 0.50834725, 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.80714346, 0.63422314,\n",
       "        0.37722751, 0.29351266, 0.        , 0.        , 1.        ,\n",
       "        0.77518754, 1.        , 0.90691754, 1.        , 0.72021502,\n",
       "        0.31076104, 0.80714346, 0.50075942, 0.07044722, 0.17319785,\n",
       "        0.16817449, 0.56571637, 0.        , 0.63422314, 0.80714346,\n",
       "        0.19710354, 0.        , 0.        , 0.23696135, 0.61448696,\n",
       "        0.29351266, 0.58789407, 1.        , 0.69447261, 1.        ,\n",
       "        0.7306701 , 0.35060109, 0.23696135, 0.58789407, 0.5035791 ,\n",
       "        0.71439873, 0.39076955, 0.80714346, 0.4075365 , 0.45242837,\n",
       "        0.27362543, 0.50278396, 0.63558485, 0.63558485, 0.34658256,\n",
       "        0.28152958, 0.16297442, 0.42480469, 0.50119332, 0.36173615,\n",
       "        0.50147406, 0.75655173, 0.57630769, 0.50092816, 0.        ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 1.        , 0.        , 0.        , 1.        ,\n",
       "        0.50834725, 0.37722751, 0.50834725, 0.69447261, 1.        ,\n",
       "        1.        , 1.        , 0.76872501, 0.80714346, 0.87904185,\n",
       "        0.68199203, 0.59721981, 0.82173159, 0.82371024, 0.74185955,\n",
       "        0.50847138, 1.        ])]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_predictor.text_embedder.target_encoder.encodings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object PredictorWithTextEmbeddings.fit at 0x2a3e2fbc0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PredictorWithTextEmbeddings(api_key=\"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\",\n",
    "                                    base_predictor=TabPFNClassifier() if task == \"classification\" \n",
    "                                    else TabPFNRegressor()).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabpfn_client_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
