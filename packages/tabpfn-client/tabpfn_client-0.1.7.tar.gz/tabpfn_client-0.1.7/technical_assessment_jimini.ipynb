{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Technique Jimini\n",
    "\n",
    "## Présentation de Jimini\n",
    "\n",
    "Pour rappel, nous développons trois outils : \n",
    "- Un outil d'analyse factuelle de documents;\n",
    "- Un outil de recherche juridique;\n",
    "- Un outil de rédaction de documents juridiques, type contrats.\n",
    "\n",
    "## Information Retrieval\n",
    "\n",
    "### Objectif\n",
    "\n",
    "Les deux premiers outils nécessitent d'être capable de récupérer le contexte pertinent dans un ensemble de paragraphes donné - problématique connue sous le nom d'`information retrieval` ou de `document retrieval`. L'objectif de cette partie est donc de construire un moteur de recherche juridique. Nous disposons d'un corpus de documents juridiques, et nous souhaitons que l'utilisateur puisse poser une question en langage naturel, puis que le moteur de recherche lui renvoie les documents les plus pertinents.\n",
    "\n",
    "### Dataset \n",
    "\n",
    "Dans le cadre de ce notebook, nous nous intéressons à un jeu de données juridique francophone : [Belgian Statuatory Article Retrieval Dataset (BSARD)](https://huggingface.co/datasets/maastrichtlawtech/bsard), qui contient ~1000 questions posées par les utilisateurs d'une plateforme d'information juridique belge (Droits Quotidiens), ainsi que les articles pertinents pour y répondre. Ce jeu de données est issu de l'article [A Statutory Article Retrieval Dataset in French](https://arxiv.org/pdf/2108.11792.pdf) de l'Université de Maastricht.\n",
    "\n",
    "Ainsi que détaillé sur la page HuggingFace, le dataset se présente sous la forme de deux fichiers CSV contenant les questions d'entraînement `questions_fr_train.csv` et de test `questions_fr_test.csv`, et structurés comme suit :\n",
    " - `id` : un attribut int32 correspondant à un numéro d'identifiant unique pour la question.\n",
    " - `question` : un attribut de type chaîne de caractères correspondant à la question.\n",
    " - `category` : un attribut de type chaîne de caractères correspondant au sujet général de la question.\n",
    " - `subcategory` : un attribut de type chaîne de caractères correspondant au sous-sujet de la question.\n",
    " - `extra_description` : un attribut de type chaîne de caractères correspondant aux tags de catégorisation supplémentaires de la question.\n",
    " - `article_ids` : un attribut de type chaîne de caractères contenant les ID d'articles pertinents pour la question, séparés par des virgules.\n",
    "\n",
    "(À noter, ce dataset contient également un échantillon de ~113k questions synthétiques, ainsi que des exemples de *négatifs* durs, c'est-à-dire des questions avec des articles non-pertinents, dont vous pouvez librement vous servir pour entraîner votre modèle.)\n",
    "\n",
    "Et d'un fichier `articles.csv` :\n",
    " - `id` : un attribut int32 correspondant à un numéro d'identifiant unique pour l'article.\n",
    " - `article` : un attribut de type chaîne de caractères correspondant à l'intégralité de l'article.\n",
    " - `code` : un attribut de type chaîne de caractères correspondant au code de loi auquel appartient l'article.\n",
    " - `article_no` : un attribut de type chaîne de caractères correspondant au numéro de l'article dans le code.\n",
    " - `description` : un attribut de type chaîne de caractères correspondant aux titres concaténés de l'article.\n",
    " - `law_type` : un attribut de type chaîne de caractères dont la valeur est soit \"regional\" soit \"national\".\n",
    "\n",
    "### Évaluation\n",
    "\n",
    "Ainsi que dans l'[article](https://arxiv.org/pdf/2108.11792.pdf), nous évaluerons la performance du modèle en utilisant les métriques R@100, R@200, R@500, MAP@100 et MRR@100.\n",
    "\n",
    "Cependant, en conditions réelles, nous souhaitons que le moteur de recherche renvoie les documents les plus pertinents en premier. Nous utiliserons donc la métrique NDCG@100, qui prend en compte l'ordre des documents renvoyés, ainsi que les métriques précédentes, avec un $k$ plus petit (par exemple $k=1$, $k=5$ et $k=10$).\n",
    "\n",
    "#### Pyterrier\n",
    "\n",
    "On se sert ici du framework Pyterrier qui permet assez simplement de prototyper et évaluer un moteur de recherche, mais **libre au candidat d'utiliser l'approche qu'il préfère**.\n",
    "Dans le code suivant, on se sert des notations de `PyTerrier`, qui fonctionne avec les clés suivantes :\n",
    "- `qid` : `str` l'identifiant de la question\n",
    "- `docno` : `str` l'identifiant de l'article\n",
    "- `query` : `str` le texte de la question\n",
    "- `rel_docnos` : `List[str]` la liste des documents pertinents pour la question\n",
    "- `score`: `float` le score de pertinence du document pour la question\n",
    "- `label`: `int` la pertinence du document pour la question (1 si pertinent, 0 sinon)\n",
    "- `qrels` : `Dict[str, Dict[str, int]]` un dictionnaire contenant les documents pertinents pour chaque question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets==2.15.0 --quiet\n",
    "!pip install pandas==2.1.3 --quiet\n",
    "!pip install python-terrier==0.10.0 --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the corpus of articles from the dataset\n",
    "!wget https://huggingface.co/datasets/maastrichtlawtech/bsard/resolve/main/articles.csv --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import pyterrier as pt\n",
    "from pyterrier.measures import *\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "datasets = load_dataset(\"maastrichtlawtech/bsard\")\n",
    "train, test = datasets['train'], datasets['test']\n",
    "\n",
    "# Convert to pandas dataframes\n",
    "df_train = pd.DataFrame(train)\n",
    "df_test = pd.DataFrame(test)\n",
    "df_articles = pd.read_csv('articles.csv')\n",
    "\n",
    "# Ensure that the article IDs are strings, and rename the column to 'docno', because of PyTerrier's expectations\n",
    "df_articles['docno'] = df_articles['id'].astype(str)\n",
    "df_articles[\"article\"] = df_articles[\"article\"].str.replace(\"[^a-zA-Z0-9 À-ÿ]\", \" \", regex=True).str.strip().str.replace(\" +\", \" \", regex=True)\n",
    "\n",
    "# Define a function to prepare the query dataframe\n",
    "def prepare_queries(df):\n",
    "    queries = pd.DataFrame({\n",
    "        'qid': df['id'].astype(str),\n",
    "        'query': df['question'].str.replace(\"[^a-zA-Z0-9 À-ÿ]\", \" \", regex=True).str.strip().str.replace(\" +\", \" \", regex=True),\n",
    "        'rel_docnos': [str(_id) for _id in df['article_ids']]\n",
    "    })\n",
    "    return queries\n",
    "\n",
    "# Prepare the train and test query dataframes\n",
    "train_queries = prepare_queries(df_train)\n",
    "test_queries = prepare_queries(df_test)\n",
    "\n",
    "# Convert relevance judgements to a DataFrame, for PyTerrier\n",
    "qrels = pd.DataFrame([\n",
    "    {'qid': str(qid), 'docno': str(docno), 'label': 1}\n",
    "    for qid, docnos in test_queries[['qid', 'rel_docnos']].itertuples(index=False)\n",
    "    for docno in ast.literal_eval(docnos)  # Convert the string representation of list back to a list\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On indexe les articles dans l'index PyTerrier (`DFIndex` avait un bug, donc on passe par un `IterDictIndex`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing the articles using IterDictIndexer\n",
    "if os.path.exists(\"./bsard_index\"):\n",
    "    shutil.rmtree(\"./bsard_index\")\n",
    "indexer = pt.IterDictIndexer(\"./bsard_index\", overwrite=True)\n",
    "docs = [{\"docno\": str(row[\"id\"]), \"text\": row[\"article\"]} for _, row in df_articles.iterrows()]\n",
    "indexref = indexer.index(docs, fields=[\"text\"])\n",
    "\n",
    "index = pt.IndexFactory.of(indexref)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le `retriever` doit être capable, étant donné une question, de classer les articles par pertinence. Le format de sortie est un CSV avec trois colonnes :\n",
    "- `qid` : `str` l'identifiant de la question \n",
    "- `docno` : `str` l'identifiant de l'article\n",
    "- `score` : `float` le score de pertinence de l'article pour la question\n",
    "\n",
    "Libre au candidat de choisir l'approche qu'il préfère pour construire le `retriever`, l'essentiel étant qu'il construise un fichier CSV avec les colonnes `qid`, `docno` et `score`.\n",
    "\n",
    "**Remarque** : Il peut ainsi y avoir plusieurs lignes pour une même question, si plusieurs articles sont pertinents (ce qui est souvent le cas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the retrieval model : for example dummy BM25 and TF-IDF\n",
    "tfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\", properties={\"c\": 1.0})\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\",  properties={\"c\": 1.0, \"bm25.k_1\": 1, \"bm25.b\": 0.6})\n",
    "\n",
    "# Run the TF-IDF model and save the results\n",
    "tfidf_results = tfidf.transform(test_queries)\n",
    "tfid_results = tfidf_results[[\"qid\", \"docno\", \"score\"]]\n",
    "tfidf_results.to_csv(\"tfidf_results.csv\", index=False)\n",
    "\n",
    "# Run the BM25 model and save the results\n",
    "bm25_results = bm25.transform(test_queries)\n",
    "bm25_results = bm25_results[[\"qid\", \"docno\", \"score\"]]  \n",
    "bm25_results.to_csv(\"bm25_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     name  recall_1  recall_5  recall_10     RR@10     AP@10  recall_100   \n",
      "0  TF-IDF  0.062664  0.149757   0.208388  0.189779  0.108045    0.462643  \\\n",
      "1    BM25  0.047595  0.088109   0.119498  0.131880  0.069838    0.350239   \n",
      "\n",
      "   recall_200  recall_500       map      ndcg    RR@100  nDCG@100    AP@100  \n",
      "0    0.523497    0.583612  0.127457  0.254490  0.201457  0.221742  0.125681  \n",
      "1    0.443217    0.510365  0.086245  0.200741  0.143892  0.159139  0.083980  \n"
     ]
    }
   ],
   "source": [
    "# Define the evaluation metrics\n",
    "eval_metrics = [\"recall_1\", \"recall_5\", \"recall_10\", RR@10, AP@10, \"recall_100\", \"recall_200\", \"recall_500\", \"map\", \"ndcg\", RR@100, nDCG@100, AP@100]\n",
    "\n",
    "# Load the results\n",
    "tfidf_results = pd.read_csv(\"tfidf_results.csv\")\n",
    "bm25_results = pd.read_csv(\"bm25_results.csv\")\n",
    "\n",
    "# Evaluate the models\n",
    "result = pt.Experiment(\n",
    "    [tfidf_results, bm25_results],\n",
    "    test_queries,\n",
    "    qrels,\n",
    "    eval_metrics,\n",
    "    names=['TF-IDF', \"BM25\"],\n",
    ")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quelques questions ouvertes\n",
    "\n",
    "### Information Retrieval\n",
    "\n",
    "Dans le cas de *Jimini Analyzer*, on est dans un contexte un peu particulier, où l'information provient de documents structurés.\n",
    "- L'approche précédente ne tient compte que du contenu des paragraphes, pas de leur place dans la structure du document. Comment pourrait-on améliorer cela ?\n",
    "- Que pensez-vous du dataset utilisé ? Comment pourrait-on l'améliorer ?\n",
    "- Quelle stratégie adopter pour faire un résumé d'un long document ?\n",
    "\n",
    "\n",
    "### Raisonnement juridique\n",
    "\n",
    "\n",
    "Récupérer le contexte pertinent est une étape *sine qua none*, mais il faut ensuite fournir le contexte au LLM, et qu'il réponde à la question posée. \n",
    "- Comment évaluer le LLM sur le contenu de ses réponses ? \n",
    "- Quelles métriques utiliser ? \n",
    "- Quel dataset utiliser / construire pour faire acquérir un raisonnement juridique au LLM ?\n",
    "- Comment s'assurer que le dataset est de qualité / représentatif ?  \n",
    "- De quel modèle partir ? \n",
    "- Quelle technique d'entraînement utiliser ? (full-parameter fine-tuning, PEFT - LoRA - QLoRA , prompt-tuning,...)\n",
    "- Comment contrôler l'hallucination ?\n",
    "- Comment faire de l'amélioration continue, en s'assurant que le modèle ne se dégrade pas lorsqu'on lui enseigne une nouvelle compétence ?\n",
    "- Comment limiter les frais de calculs ?\n",
    "\n",
    "\n",
    "### Rédaction de documents\n",
    "\n",
    "- Comment rédiger un contrat de plusieurs dizaines de pages ?\n",
    "\n",
    "### Bonus\n",
    "\n",
    "Sentez-vous libre de proposer des améliorations, des idées, des pistes, etc. !\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
