{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Analyze a post-training Bernoulli-Bernoulli RBM\n\nThis script shows how to analyze the RBM after having trained it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport torch\n\ndevice = torch.device(\"cpu\")\ndtype = torch.float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the dataset\nWe suppose the RBM was trained on the `dummy.h5` dataset file, with 60% of the train dataset.\nBy default, the dataset splitting is seeded. So just putting the same train_size and test_size ensures\nhaving the same split for analysis. This behaviour can be changed by setting a different value to the `seed` keyword.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from rbms.dataset import load_dataset\n\ntrain_dataset, test_dataset = load_dataset(\n    \"dummy.h5\", train_size=0.6, test_size=0.4, device=device, dtype=dtype\n)\nnum_visibles = train_dataset.get_num_visibles()\n\nU_data, S_data, V_dataT = torch.linalg.svd(\n    train_dataset.data - train_dataset.data.mean(0)\n)\nproj_data = train_dataset.data @ V_dataT.mT / num_visibles**0.5\nproj_data = proj_data.cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the model.\nFirst, we want to know which machines have been saved\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from rbms.utils import get_saved_updates\n\nfilename = \"RBM.h5\"\nsaved_updates = get_saved_updates(filename=filename)\nprint(f\"Saved updates: {saved_updates}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will load the last saved model as well as the permanent chains during training\nOnly the configurations associated to the last saved model have been saved for the permanent chains.\nWe also get access to the hyperparameters of the RBM training as well as the time elapsed during the training.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from rbms.io import load_model\n\nparams, permanent_chains, training_time, hyperparameters = load_model(\n    filename=filename, index=saved_updates[-1], device=device, dtype=dtype\n)\n\nprint(f\"Training time: {training_time}\")\nfor k in hyperparameters.keys():\n    print(f\"{k} : {hyperparameters[k]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To follow the training of the RBM, let's look at the singular values of the weight matrix\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from rbms.utils import get_eigenvalues_history\n\ngrad_updates, sing_val = get_eigenvalues_history(filename=filename)\n\nfig, ax = plt.subplots(1, 1)\nax.plot(grad_updates, sing_val)\nax.set_xlabel(\"Training time (gradient updates)\")\nax.set_ylabel(\"Singular values\")\nax.loglog()\nfig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's compare the permanent chains to the dataset distribution. To do so, we project the chains on the first\nprincipal components of the dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from rbms.plot import plot_PCA\n\nproj_pc = permanent_chains[\"visible\"] @ V_dataT.mT / num_visibles**0.5\n\nplot_PCA(\n    proj_data,\n    proj_pc.cpu().numpy(),\n    labels=[\"Dataset\", \"Permanent chains\"],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample the RBM\nAnother interesting thing is to compare generated samples starting from random configurations\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from rbms.sampling.gibbs import sample_state\n\nnum_samples = 2000\nchains = params.init_chains(num_samples=num_samples)\nproj_gen_init = chains[\"visible\"] @ V_dataT.mT / num_visibles**0.5\nplot_PCA(\n    proj_data,\n    proj_gen_init.cpu().numpy(),\n    labels=[\"Dataset\", \"Starting position\"],\n)\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now sample those chains and compare again the distribution\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_steps = 100\nchains = sample_state(gibbs_steps=n_steps, chains=chains, params=params)\n\nproj_gen = chains[\"visible\"] @ V_dataT.mT / num_visibles**0.5\nplot_PCA(\n    proj_data,\n    proj_gen.cpu().numpy(),\n    labels=[\"Dataset\", \"Generated samples\"],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the AIS estimation of the log-likelihood.\nFor now, we only looked at a qualitative evaluation of the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from rbms.partition_function.ais import compute_partition_function_ais\nfrom rbms.utils import compute_log_likelihood\n\nlog_z_ais = compute_partition_function_ais(num_chains=2000, num_beta=100, params=params)\n\nprint(\n    compute_log_likelihood(\n        train_dataset.data, train_dataset.weights, params=params, log_z=log_z_ais\n    )\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}