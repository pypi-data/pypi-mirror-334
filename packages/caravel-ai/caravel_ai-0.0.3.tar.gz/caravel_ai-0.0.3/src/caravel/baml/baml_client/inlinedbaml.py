###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml-py
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off

file_map = {
    
    "APIUtil_legacy.baml": "\ntemplate_string NoMarkdown() #\"\n    Do not use markdown.\n\"#\n\ntemplate_string NotEnoughContextError(error_message: string, required: string[]) #\"\n    NotEnoughContextError: {{ error_message }}, required: {{ required }}\n\"# \n\ntemplate_string EmptyDictString() #\"\n    {}\n\"#\n\ntemplate_string EmptyString() #\"\n    \n\"# \n\n\n\nfunction GenerateHumanLanguageResponse(json: string, context: string) -> string {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n\n        Describe the content within the following\n        {{ json }}\n        as if you are answering the prompt in\n        {{ context }}\n        in a single paragraph. Do not include technical details, just describe the content itself.\n        \n        {{ ctx.output_format }}\n    \"#\n} // GenerateHumanLanguageResponse\n\ntest new_function {\n    functions [PopulateQueryParameters]\n    args {\n        fmt {\n            \"start_cursor\" \"\"\n            \"filter[acquisition_type][eq]\" \"loan | lease\"\n            \"filter[reference_number][like]\" \"\"\n            \"filter[vehicle_id][eq]\" \"\"\n            \"filter[lender_id][eq]\" \"\"\n            \"filter[vendor_id][eq]\" \"\"\n            \"filter[acquisition_date][lt]\" \"\"\n            \"filter[created_at][lt]\" \"\"\n            \"filter[updated_at][lt]\" \"\"\n            \"sort[id]\" \"desc\"\n            \"sort[acquisition_date]\" \"asc | desc\" \n            \"sort[created_at]\" \"asc | desc\"\n            \"sort[updated_at]\" \"asc | desc\"\n        }\n        context #\"\n            Get all of my loan acquisitions from 2010 to 2020 sorted in chronological order.\n        \"#\n    }\n}\n\n\n\n",
    "ContextManager.baml": "\nenum Role {\n    USER @description(#\"\n        The end user of the application.\n    \"#)\n    TOOL @description(#\"\n        A function call or API request.\n    \"#)\n    SYSTEM @description(#\"\n        The system prompt.\n    \"#)\n    BOT  @description(#\"\n        A human-readable message generated by the LLM for direct consumption by the end-user USER.\n    \"#)\n} // Role\n\nenum State {\n    INITIAL @description(#\"\n        The initial state of the context when an assistant is instantiated.\n    \"#)\n    AWAITING_USER @description(#\"\n        BOT is awaiting user input. This will always be set manually and should not be estimated.\n    \"#)\n    MAKE_REQUEST @description(#\"\n        A state where the user is seeking to make a request, the bot has explained the requirements of the request body, and the user has given them.\n    \"#)\n    EXPLAIN_REQUEST @description(#\"\n        A state where the user wants to make a request, but the BOT has not yet told the user the requirements for the request or recieved a tool call explaining requirements from the request. The bot should inform them of the options of the request and ask them if they wish to proceed. If yes, then the state should become MAKE_REQUEST.\n    \"#)\n    NATURAL_LANGUAGE @description(#\"\n        A state where the user has asked something that can be answered without interacting with the API or TOOL has returned a message. Try to be as conversational as possible and don't unnecessarily repeat yourself.\n    \"#)\n} // State\n\nclass Context {\n    state State? @description(#\"\n        The current state that the program is in. Used to guide intent throughout the flow.\n    \"#)\n    messages Message[]? @description(#\"\n        The messages between the system, user, bot, and function calls.\n    \"#)\n    limit int?\n} // Context\n\nclass Message {\n    role Role @description(#\"The role of the message creator.\"#)\n    context_state State @description(#\"\n        The state of the context when this message was created.\n    \"#)\n\n    content string  @description(#\"\n        The content created by the entity described in role.\n    \"#)\n} // Message\n\nfunction GenerateNaturalLanguageResponse(context: Context) -> string {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        Using {{ context }}, generate a human language response that will help the user. Give precedence to more recent messages. If the user has expressed interest in a request, and the state has been explain request, but not make request, then ask the user if they would like to proceed with the request.\n        {{ ctx.output_format }}\n    \"#\n} // GenerateNaturalLanguageResponse\n\nfunction EstimateState(msgs: Message[]) -> State {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        Using the messages that have led up to now (giving precedence to more recent messages):\n        {{ msgs }},\n        {{ ctx.output_format }}\n        Never evaluate to AWAITING_USER.\n    \"#\n} // EstimateState\n\n\ntest est_state_00 {\n    functions [EstimateState]\n    args {\n        // current_state AWAITING_USER\n        msgs [\n            {\n                role SYSTEM\n                content \"You are a helpful AI Assistant. Help the user accomplish the tasks they ask you for help on.\"\n            }\n            {\n                role USER\n                content \"Hi there. Can you tell me about large language models?\"\n            }\n        ]\n    }\n}\n\ntest est_state_01 {\n    functions [EstimateState]\n    args {\n        // current_state NATURALLANGUAGE\n        msgs [\n            {\n                role SYSTEM\n                content \"You are a helpful AI Assistant. Help the user accomplish the tasks they ask you for help on.\"\n            }\n            {\n                role USER\n                content \"Hi there. Can you tell me about large language models?\"\n            }\n            {\n                role BOT\n                content \"Hello! Large language models are AI models build on top of transformer architectures that use large input parameters to predict which word comes next. They are the most advanced NLP models in existence. Is there anything in particular that you want to learn about?\"\n            }\n        ]\n    }\n}\n\ntest est_state_02 {\n    functions [EstimateState]\n    args {\n        msgs [\n            {\n                role SYSTEM\n                content \"You are a helpful AI Assistant. Help the user accomplish the tasks they ask you for help on.\"\n            }\n            {\n                role USER\n                content \"Hi there. Can you tell me about large language models?\"\n            }\n            {\n                role BOT\n                content \"Hello! Large language models are AI models build on top of transformer architectures that use large input parameters to predict which word comes next. They are the most advanced NLP models in existence. Is there anything in particular that you want to learn about?\"\n            }\n            {\n                role USER\n                content \"No, thanks though. I want to add a vehicle to fleetio.\"\n            }\n        ]\n    }\n}\n\n// this case may never happen though, right?\ntest est_state_03 {\n    functions [EstimateState]\n    args {\n        msgs [\n            {\n                role SYSTEM\n                content \"You are a helpful AI Assistant. Help the user accomplish the tasks they ask you for help on.\"\n            }\n            {\n                role USER\n                content \"Hi there. Can you tell me about large language models?\"\n            }\n            {\n                role BOT\n                content \"Hello! Large language models are AI models build on top of transformer architectures that use large input parameters to predict which word comes next. They are the most advanced NLP models in existence. Is there anything in particular that you want to learn about?\"\n            }\n            {\n                role USER\n                content \"No, thanks though. I want to add a vehicle to fleetio.\"\n            }\n            {\n                role TOOL\n                content #\"\n                    request_body {\n                        data {\n                            'vehicle_status_id': int\n                            'primary_meter_unit': `km` or `mi` or `hr`\n                            'make': string or null\n                            'name': string\n                        }\n                    }\n                \"#\n            }\n        ]\n    }\n}\n\n\n",
    "DynamicAPI.baml": "\nfunction GenerateIntent(intents: string[], intent: Context) -> string @description(#\"\n    A string formatted in the following manner: <METHOD Intent>, where METHOD is the HTTP Request that will need to be used. \n\"#) {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        Using the following list of intents, \n        {{ intents }}\n        and the current context of the conversation:\n        {{ intent }}\n        output the intent, sans quotes.\n        {{ ctx.output_format }}   \n    \"#\n} // GetIntent\n\nfunction GeneratePath(path: string, context: Context) -> string @description(#\"\n    A string representing the altered path, filled with the path parameters based on the user input.\n\"#) {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        Using this path outline and extracting any path arguments ({id}, :id, etc):\n        (Note: Never put query parameters into the path, only path parameters)\n        {{ path }}\n        Generate the path with path parameters (if any exist) filled in, using information gleaned from this input:\n        {{ context }}\n        If the parameter cannot be inferred, return:\n        {{ NoParamProvided() }}\n         If no path parameter exists within {{ path }}, return only:\n        {{ path }}\n        {{ ctx.output_format }}\n    \"#\n} // MakePath\n\n// A dynamic class that will be used for checking the schema of OpenAPI specifications\nclass DynamicObject {\n    @@dynamic\n} // DynamicObject\n\n// A dynamic enum that will be used for constructing dynamic json objects.\nenum DynamicEnum {\n    @@dynamic\n} // DynamicEnum\n\nfunction ExtractDynamicTypes(context: Context) -> DynamicObject {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        {{ ctx.output_format }}\n        {{ _.role('user') }}\n        Using the context from the conversation:\n        {{ context }}\n        Sometimes, you may need to coerce a user provided input into the nearest allowed value for an attribute (e.g. If allowed options are `mi`, `km`, `hr`, and user supplies `miles`, coerce to `mi`)\n    \"#\n} // ExtractDynamicTypes\n\nenum HTTPMethod {\n    GET\n    DELETE\n    POST\n    PATCH\n    PUT\n}\n\n// An API request object\nclass DynamicAPIRequest {\n    path string\n    method HTTPMethod\n    params  map<string, string>? @description(#\"\n        Query Parameters. Will usually be null for POST and PATCH requests.\n    \"#)\n    request_body DynamicObject? @description(#\"\n        A dynamically generated request body based on user input. It will be \n    \"#)\n} // DynamicAPIRequest \n\n// map<string, string> works here because query paramas are ALWAYS strs\nfunction GenerateQueryParameters(fmt: map<string, string | int | float | bool>, context: Context) -> map<string, string> {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        DO NOT HALLUCINATE QUERY PARAMS NOT IN {{ fmt }}\n\n        Using the following query parameter structure:\n        {{ fmt }}\n        And the provided context: {{ context }},\n        Complete the query parameters. If the query param structure is empty, return {{ fmt }} with appropriate default values populated.\n        {{ ctx.output_format }}\n    \"#\n} // newPopulateQP\n\n\n// A function for constructing a dynamic API request as specified above.\nfunction ConstructDynamicAPIRequest(path: string, method: HTTPMethod | string, params: map<string, string>?, request_body: DynamicObject?) -> DynamicAPIRequest {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        Create an API Request with the given parameters.\n        {{ ctx.output_format }}\n        Path: {{ path }}\n        method: {{ method }}\n        query parameters: {{ params }}\n        request_body: {{ request_body }}\n    \"#\n} // ConstructDynamicAPIRequest\n\n\n",
    "Intent_legacy.baml": "\ntest get_intent {\n    functions [GetIntent]\n    args {\n        intents [\n            \"POST Add a new vehicle to Fleetio\",\n            \"GET Get a vehicle from Fleetio\",\n            \"PATCH Edit a vehicle in Fleetio\" \n        ]\n        intent #\"upload a new vehicle to fleetio\"#\n    } // args\n} // get_intent",
    "MakePath_legacy.baml": "\ntemplate_string NoParamProvided() #\"\n    NoParamProvidedError\n\"# // NoParamProbided\n\ntest make_path_param_given {\n    functions [MakePath]\n    args {\n        path #\"/v1/vehicles/{id}\"#\n        user_input #\"Get vehicle with id 1234\"#\n    }\n}\n\ntest make_path_param_not_given {\n    functions [MakePath]\n    args {\n        path #\"/v1/vehicles/{id}\"#\n        user_input #\"Can you get my dad's truck from fleetio? \"#\n    }\n}\n\ntest make_path_no_path_param_present {\n    functions [MakePath]\n    args {\n        path #\"/v1/vehicles/\"#\n        user_input #\"Can you give me all of my vehicles in fleetio?\"#\n    }\n}\n\ntest make_path_no_path_param_present_but_some_given {\n    functions [MakePath]\n    args {\n        path \"/pet\"\n        user_input \"Add a new pet to the store named blue. It is a 2 year old dog!\"\n    }\n}\n\n",
    "ParsingTransform_unused.baml": "\nfunction CreateDescription(current_api_description: string, descriptions: string[]) -> string {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        Using the following description:\n        {{ current_api_description }}\n        Output a single-sentence description that makes sense in human language. Be sure the description you create does not already exist here:\n        {{ descriptions }}\n        {{ ctx.output_format}}\n    \"#\n} // CreateDescription\n\ntest create_description_test {\n    functions [CreateDescription]\n    args {\n        current_api_description #\"\"Cancels an Account that belongs to your organization.\\n\\n:::info\\nThis endpoint is only usable by Fleetio partners with an Organization Token or Partner Token.\\n:::\",\"#\n        descriptions []\n    }\n}",
    "Retry.baml": "\nfunction RetryFailedAPIRequest(err: string @assert(equal_to_500, {{this == \"500\"}}), context: string, original_api_req: DynamicAPIRequest) -> DynamicAPIRequest {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        {{original_api_req}}\n    \"#\n\n    // how do I recreate the DynamicAPIRequest\n}",
    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    mutliplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.78.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode async\n}\n",
}

def get_baml_files():
    return file_map