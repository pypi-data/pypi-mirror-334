Metadata-Version: 2.1
Name: mombai
Version: 2.0.3
Summary: A deep learning library for advanced neural network layers.
Home-page: https://github.com/joaquinsc999/mombai
Author: Joaquín Francisco Solórzano Corea
Author-email: joaquinscorea@gmail.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: tensorflow>=2.0

# Mombai

Mombai es una librerÃ­a de aprendizaje profundo diseÃ±ada para implementar y experimentar con capas de redes neuronales avanzadas, basada en investigaciones recientes. Esta librerÃ­a incluye implementaciones de Kolmogorov-Arnold Networks (KANs) y modelos innovadores como **WAFNet** y **MAXNet**, los cuales exploran nuevas formas de combinar y seleccionar funciones de activaciÃ³n dinÃ¡micamente.

## MotivaciÃ³n

El proyecto Mombai nace de la necesidad de explorar y llevar a la prÃ¡ctica conceptos avanzados de redes neuronales presentados en papers de investigaciÃ³n recientes. La librerÃ­a estÃ¡ en sus primeras fases de desarrollo y busca proporcionar herramientas para experimentar con activaciones hÃ­bridas y mecanismos de atenciÃ³n.

A futuro, se espera mejorar la eficiencia de estas implementaciones e incorporar nuevas capas basadas en experimentos innovadores en inteligencia artificial.

## InstalaciÃ³n

Puedes instalar la librerÃ­a directamente desde PyPI usando pip:

```bash
pip install mombai
```

## Uso

### 1. Uso de `KANLayer`
Ejemplo de cÃ³mo usar la capa `KANLayer` para entrenar un modelo simple que ajuste la funciÃ³n y = 3x + 2:

```python
import tensorflow as tf
from mombai.layers.kan import KANLayer

class KANModel(tf.keras.Model):
    def __init__(self, units=1):
        super(KANModel, self).__init__()
        self.kan_layer = KANLayer(units=units, G=5, k=3)
        self.output_layer = tf.keras.layers.Dense(1)

    def call(self, inputs):
        x = self.kan_layer(inputs)
        return self.output_layer(x)

# GeneraciÃ³n de datos
x_train = tf.random.uniform((1000, 1), -1, 1)
y_train = 3 * x_train + 2

# Crear y entrenar el modelo
model = KANModel(units=10)
model.compile(optimizer='adam', loss='mse')
model.fit(x_train, y_train, epochs=10, batch_size=32)

# Probar el modelo
x_test = tf.constant([[0.5]], dtype=tf.float32)
y_pred = model.predict(x_test)
print(f"PredicciÃ³n para x=0.5: {y_pred}")
```

---

## 2. Uso de `WAFLayer`
`WAFLayer` es una capa que permite combinar mÃºltiples funciones de activaciÃ³n en una sola transformaciÃ³n de los datos. Se pueden sumar o promediar las activaciones antes de aplicarlas.

Ejemplo de uso en una red densa con compresor de suma:

```python
import tensorflow as tf
from mombai.layers.multi_activation import WAFLayer

model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, input_shape=(10,)),
    WAFLayer(units=32, activations=['relu', 'swish', 'gelu'], compressor="sum"),
    tf.keras.layers.Dense(1)
])

model.compile(optimizer='adam', loss='mse')
model.summary()
```

Ejemplo de uso despuÃ©s de una capa convolucional con compresor de promedio:

```python
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D((2, 2)),
    WAFLayer(units=64, activations=['relu', 'swish', 'elu'], compressor="avg"),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()
```

---

---

## 3. Uso de `WAFLayerNorm`
`WAFLayerNorm` es una capa que permite combinar mÃºltiples funciones de activaciÃ³n en una sola transformaciÃ³n de los datos. Se pueden sumar o promediar las activaciones antes de aplicarlas y se aplica una normalizaciÃ³n para controlar el aporte de cada funciÃ³n.

Ejemplo de uso en una red densa con compresor de suma:

```python
import tensorflow as tf
from mombai.layers.multi_activation import WAFLayerNorm

model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, input_shape=(10,)),
    WAFLayerNorm(units=32, activations=['relu', 'swish', 'gelu'], compressor="sum"),
    tf.keras.layers.Dense(1)
])

model.compile(optimizer='adam', loss='mse')
model.summary()
```

Ejemplo de uso despuÃ©s de una capa convolucional con compresor de promedio:

```python
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D((2, 2)),
    WAFLayerNorm(units=64, activations=['relu', 'swish', 'elu'], compressor="avg"),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()
```

---

## 3. Uso de `MAXLayerWithAttention`
Esta capa permite que la red **seleccione dinÃ¡micamente** quÃ© funciones de activaciÃ³n usar en cada entrada mediante un mecanismo de atenciÃ³n.

Ejemplo de uso en una red:

```python
import tensorflow as tf
from mombai.layers.multi_activation import MAXLayerWithAttention

model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, input_shape=(10,)),
    MAXLayerWithAttention(units=32, activations=['relu', 'swish', 'gelu']),
    tf.keras.layers.Dense(1)
])

model.compile(optimizer='adam', loss='mse')
model.summary()
```

---

## 4. Uso de `MAXLayerWithSelfAttention`
En esta variante, la capa usa **autoatenciÃ³n** para ajustar la combinaciÃ³n de activaciones en funciÃ³n del contexto de la entrada.

Ejemplo en un modelo LSTM para clasificaciÃ³n de texto:

```python
import tensorflow as tf
from tensorflow.keras.layers import Embedding, LSTM, Dense
from mombai.layers.multi_activation import MAXLayerWithSelfAttention

model = tf.keras.Sequential([
    Embedding(input_dim=5000, output_dim=128, input_length=100),
    LSTM(64, return_sequences=True),
    MAXLayerWithSelfAttention(units=32, activations=['relu', 'swish', 'gelu']),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()
```

---

## Funciones consideradas en layers multi_activation
- Nota: No todas las funciones han sido probadas. Puede que exista algÃºn bug con una funciÃ³n en particular.
```json
{
  "relu": "tf.nn.relu",
  "sigmoid": "tf.nn.sigmoid",
  "tanh": "tf.nn.tanh",
  "softmax": "tf.nn.softmax",
  "softplus": "tf.nn.softplus",
  "softsign": "tf.nn.softsign",
  "elu": "tf.nn.elu",
  "selu": "tf.nn.selu",
  "swish": "tf.nn.swish",
  "gelu": "tf.nn.gelu",
  "leaky_relu": "tf.nn.leaky_relu",
  "relu6": "tf.nn.relu6",
  "hard_sigmoid": "tf.keras.activations.hard_sigmoid",
  "exponential": "tf.keras.activations.exponential",
  "linear": "tf.keras.activations.linear",
  "log_softmax": "tf.nn.log_softmax",
  "crelu": "tf.nn.crelu",
}
```


## Estado del Proyecto

Actualmente, Mombai se encuentra en una fase inicial de desarrollo, y las implementaciones estÃ¡n orientadas a experimentar con funciones de activaciÃ³n avanzadas y mecanismos de atenciÃ³n. A futuro, planeamos optimizar el rendimiento y aÃ±adir nuevas capas.

Si encuentras problemas o tienes sugerencias, no dudes en abrir un issue o contribuir al proyecto.

## Contribuciones

Las contribuciones son bienvenidas. Si quieres contribuir, revisa `CONTRIBUTING.md` (prÃ³ximamente) y asegÃºrate de que tus cambios se alineen con la direcciÃ³n general del proyecto.

## Licencia

Este proyecto estÃ¡ licenciado bajo la licencia MIT. Consulta el archivo `LICENSE` para mÃ¡s detalles.
