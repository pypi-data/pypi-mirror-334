{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import rtsvg\n",
    "rt = rtsvg.RACETrack()\n",
    "\n",
    "use_real_data = True\n",
    "\n",
    "if use_real_data:\n",
    "    df1 = pl.read_csv('../../data/2013_vast_challenge/mc3_netflow/nf/nf-chunk1.csv')\n",
    "    df2 = pl.read_csv('../../data/2013_vast_challenge/mc3_netflow/nf/nf-chunk2.csv')\n",
    "    df3 = pl.read_csv('../../data/2013_vast_challenge/mc3_netflow/nf/nf-chunk3.csv')\n",
    "    df  = pl.concat([df1, df2, df3])\n",
    "    \n",
    "    df = rt.columnsAreTimestamps(df, 'parsedDate')\n",
    "    df = df.rename({'TimeSeconds':                '_del1_',\n",
    "                    'parsedDate':                 'timestamp',\n",
    "                    'dateTimeStr':                '_del2_',\n",
    "                    'ipLayerProtocol':            'pro',\n",
    "                    'ipLayerProtocolCode':        '_del3_',\n",
    "                    'firstSeenSrcIp':             'sip',\n",
    "                    'firstSeenDestIp':            'dip',\n",
    "                    'firstSeenSrcPort':           'spt',\n",
    "                    'firstSeenDestPort':          'dpt',\n",
    "                    'moreFragments':              '_del4_',\n",
    "                    'contFragments':              '_del5_',\n",
    "                    'durationSeconds':            'dur',\n",
    "                    'firstSeenSrcPayloadBytes':   '_del6_',\n",
    "                    'firstSeenDestPayloadBytes':  '_del7_',\n",
    "                    'firstSeenSrcTotalBytes':     'soct',\n",
    "                    'firstSeenDestTotalBytes':    'doct',\n",
    "                    'firstSeenSrcPacketCount':    'spkt',\n",
    "                    'firstSeenDestPacketCount':   'dpkt',\n",
    "                    'recordForceOut':             '_del8_'})\n",
    "    df = df.drop(['_del1_', '_del2_', '_del3_', '_del4_', '_del5_', '_del6_', '_del7_', '_del8_'])\n",
    "    df = df.sample(100_000)\n",
    "else:\n",
    "    _lu_ = {'sip':[], 'dip':[], 'timestamp':[]}\n",
    "    _lu_['sip'].append('a'), _lu_['dip'].append('c'), _lu_['timestamp'].append('2022-02-01')\n",
    "    _lu_['sip'].append('e'), _lu_['dip'].append('a'), _lu_['timestamp'].append('2022-02-01')\n",
    "    _lu_['sip'].append('f'), _lu_['dip'].append('c'), _lu_['timestamp'].append('2022-02-01')\n",
    "    _lu_['sip'].append('a'), _lu_['dip'].append('z'), _lu_['timestamp'].append('2022-02-01')\n",
    "    _lu_['sip'].append('y'), _lu_['dip'].append('a'), _lu_['timestamp'].append('2022-02-01')\n",
    "\n",
    "\n",
    "    _lu_['sip'].append('a'), _lu_['dip'].append('c'), _lu_['timestamp'].append('2022-02-02')\n",
    "    _lu_['sip'].append('c'), _lu_['dip'].append('d'), _lu_['timestamp'].append('2022-02-02')\n",
    "\n",
    "    _lu_['sip'].append('a'), _lu_['dip'].append('b'), _lu_['timestamp'].append('2022-02-03')\n",
    "    _lu_['sip'].append('b'), _lu_['dip'].append('c'), _lu_['timestamp'].append('2022-02-03')\n",
    "    _lu_['sip'].append('e'), _lu_['dip'].append('a'), _lu_['timestamp'].append('2022-02-03')\n",
    "    _lu_['sip'].append('b'), _lu_['dip'].append('f'), _lu_['timestamp'].append('2022-02-03')\n",
    "\n",
    "    _lu_['sip'].append('a'), _lu_['dip'].append('b'),  _lu_['timestamp'].append('2022-02-04')\n",
    "    _lu_['sip'].append('b'), _lu_['dip'].append('c'),  _lu_['timestamp'].append('2022-02-04')\n",
    "    _lu_['sip'].append('e'), _lu_['dip'].append('a'),  _lu_['timestamp'].append('2022-02-04')\n",
    "    _lu_['sip'].append('e'), _lu_['dip'].append('m0'), _lu_['timestamp'].append('2022-02-04')\n",
    "    _lu_['sip'].append('e'), _lu_['dip'].append('m1'), _lu_['timestamp'].append('2022-02-04')\n",
    "    _lu_['sip'].append('b'), _lu_['dip'].append('f'),  _lu_['timestamp'].append('2022-02-04')\n",
    "\n",
    "    _lu_['sip'].append('a'), _lu_['dip'].append('b'), _lu_['timestamp'].append('2022-02-05')\n",
    "    _lu_['sip'].append('b'), _lu_['dip'].append('c'), _lu_['timestamp'].append('2022-02-05')\n",
    "    _lu_['sip'].append('e'), _lu_['dip'].append('a'), _lu_['timestamp'].append('2022-02-05')\n",
    "    _lu_['sip'].append('b'), _lu_['dip'].append('f'), _lu_['timestamp'].append('2022-02-05')\n",
    "    _lu_['sip'].append('a'), _lu_['dip'].append('z'), _lu_['timestamp'].append('2022-02-05')\n",
    "    _lu_['sip'].append('y'), _lu_['dip'].append('a'), _lu_['timestamp'].append('2022-02-05')\n",
    "\n",
    "    df = pl.DataFrame(_lu_)\n",
    "    df = rt.columnsAreTimestamps(df, 'timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# spreadLines() - attempt to implement this visualization\n",
    "#\n",
    "# Based on:\n",
    "#\n",
    "# @misc{kuo2024spreadlinevisualizingegocentricdynamic,\n",
    "#       title={SpreadLine: Visualizing Egocentric Dynamic Influence}, \n",
    "#       author={Yun-Hsin Kuo and Dongyu Liu and Kwan-Liu Ma},\n",
    "#       year={2024},\n",
    "#       eprint={2408.08992},\n",
    "#       archivePrefix={arXiv},\n",
    "#       primaryClass={cs.HC},\n",
    "#       url={https://arxiv.org/abs/2408.08992}, \n",
    "# }\n",
    "# \n",
    "def spreadLines(rt_self,\n",
    "                df,\n",
    "                relationships,\n",
    "                node_focus,\n",
    "                only_render_nodes    = None,  # set of nodes to render... if None, just render normally\n",
    "                ts_field             = None,  # Will attempt to guess based on datatypes\n",
    "                every                = '1d',  # \"the every field for the group_by_dynamic\" ... 1d, 1h, 1m\n",
    "                color_by             = None,\n",
    "                count_by             = None,  # does nothing\n",
    "                count_by_set         = False, # does nothing\n",
    "                node_color           = None,  # none means default color, 'vary' by color_by, or 'node' to convert the node string into a color\n",
    "                                              # ... or a dictionary of the node string to either a string to color hash or a \"#xxxxxx\"\n",
    "                alter_inter_d        = 192,       # distance between the alters\n",
    "                max_bin_w            = 64,        # max width of the bin\n",
    "                max_bin_h            = 450*2,     # max height of the bin\n",
    "                min_channel_w        = 8,         # min width of the channel\n",
    "                max_channel_w        = 16,        # max width of the channel\n",
    "                channel_inter_d      = 4,         # distance between the channels\n",
    "                r_min                = 4.0, \n",
    "                r_pref               = 7.0, \n",
    "                circle_inter_d       = 2.0, \n",
    "                circle_spacer        = 3,\n",
    "                alter_separation_h   = 48, \n",
    "                h_collapsed_sections = 16,\n",
    "                \n",
    "                prefilter_dataframe  = False,\n",
    "                widget_id            = None,\n",
    "                w                    = 1024,\n",
    "                h                    = 960,\n",
    "                x_ins                = 32,\n",
    "                y_ins                = 8,\n",
    "                txt_h                = 12):\n",
    "    if rt_self.isPolars(df) == False: raise Exception('spreadLines() - only supports polars dataframe')\n",
    "    return SpreadLines(**locals())\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "class SpreadLines(object):\n",
    "    #\n",
    "    # transform all fields (if they area t-field)\n",
    "    # - replace those fields w/ the new versions (i actually don't think the names change...)\n",
    "    #\n",
    "    def __transformFields__(self):\n",
    "        # Gather up all of the fields that are going to be used\n",
    "        _all_columns_ = [self.ts_field]\n",
    "        if self.color_by is not None: _all_columns_.append(self.color_by)\n",
    "        if self.count_by is not None: _all_columns_.append(self.count_by)\n",
    "        for _relationship_ in self.relationships:\n",
    "            _fm_, _to_ = _relationship_[0], _relationship_[1]\n",
    "            if   type(_fm_) is str: _all_columns_.append(_fm_)\n",
    "            elif type(_fm_) is tuple:\n",
    "                for i in range(len(_fm_)): _all_columns_.append(_fm_[i])\n",
    "            if   type(_to_) is str: _all_columns_.append(_to_)\n",
    "            elif type(_to_) is tuple:\n",
    "                for i in range(len(_to_)): _all_columns_.append(_to_[i])\n",
    "        # Transform the fields\n",
    "        self.df, _new_columns_ = self.rt_self.transformFieldListAndDataFrame(self.df, _all_columns_)\n",
    "        # Remap them\n",
    "        col_i = 0\n",
    "        self.ts_field        = _new_columns_[col_i]\n",
    "        col_i += 1\n",
    "        if self.color_by is not None: \n",
    "            self.color_by = _new_columns_[col_i]\n",
    "            col_i += 1\n",
    "        if self.count_by is not None:\n",
    "            self.count_by = _new_columns_[col_i]\n",
    "            col_i += 1\n",
    "        _new_relationships_ = []\n",
    "        for _relationship_ in self.relationships:\n",
    "            _fm_, _to_ = _relationship_[0], _relationship_[1]\n",
    "            if   type(_fm_) is str: \n",
    "                _fm_ = _new_columns_[col_i]\n",
    "                col_i += 1\n",
    "            elif type(_fm_) is tuple:\n",
    "                as_list = []\n",
    "                for i in range(len(_fm_)):\n",
    "                    as_list.append(_new_columns_[col_i])                    \n",
    "                    col_i += 1\n",
    "                _fm_ = tuple(as_list)\n",
    "            if   type(_to_) is str: \n",
    "                _to_ = _new_columns_[col_i]\n",
    "                col_i += 1\n",
    "            elif type(_to_) is tuple:\n",
    "                as_list = []\n",
    "                for i in range(len(_to_)): \n",
    "                    as_list.append(_new_columns_[col_i])\n",
    "                    col_i += 1\n",
    "                _to_ = tuple(as_list)\n",
    "            _new_relationships_.append((_fm_, _to_))\n",
    "        self.relationships = _new_relationships_\n",
    "\n",
    "\n",
    "    #\n",
    "    # __consolidateRelationships__() - simplify the relationship fields into a single field\n",
    "    # ... and use standard naming\n",
    "    # ... replaces the \"relationships\" field w/ the consolidated field names\n",
    "    # ... use (__fm0__, __to0__),( __fm1__, __to1__), etc.\n",
    "    #\n",
    "    def __consolidateRelationships__(self):\n",
    "        new_relationships = []\n",
    "        for i in range(len(self.relationships)):\n",
    "            _fm_, _to_ = self.relationships[i]\n",
    "            new_fm = f'__fm{i}__'\n",
    "            new_to = f'__to{i}__'\n",
    "            if type(_fm_) is str: self.df = self.df.with_columns(pl.col(_fm_).alias(new_fm))\n",
    "            else:                 self.df = self.rt_self.createConcatColumn(self.df, _fm_, new_fm)\n",
    "            if type(_to_) is str: self.df = self.df.with_columns(pl.col(_to_).alias(new_to))\n",
    "            else:                 self.df = self.rt_self.createConcatColumn(self.df, _to_, new_to)\n",
    "            new_relationships.append((new_fm, new_to))\n",
    "        self.relationships = new_relationships\n",
    "\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    def __init__(self, rt_self, **kwargs):\n",
    "        self.rt_self             = rt_self\n",
    "        self.df                  = rt_self.copyDataFrame(kwargs['df'])\n",
    "        self.relationships       = kwargs['relationships']\n",
    "        self.node_focus          = kwargs['node_focus']\n",
    "        self.only_render_nodes   = kwargs['only_render_nodes']\n",
    "        self.ts_field            = self.rt_self.guessTimestampField(self.df) if kwargs['ts_field'] is None else kwargs['ts_field']\n",
    "        self.every               = kwargs['every']\n",
    "        self.color_by            = kwargs['color_by']\n",
    "        self.count_by            = kwargs['count_by']\n",
    "        self.count_by_set        = kwargs['count_by_set']\n",
    "        self.node_color          = kwargs['node_color']\n",
    "\n",
    "        self.alter_inter_d        = kwargs['alter_inter_d']\n",
    "        self.max_bin_w            = kwargs['max_bin_w']\n",
    "        self.max_bin_h            = kwargs['max_bin_h']\n",
    "        self.min_channel_w        = kwargs['min_channel_w']\n",
    "        self.max_channel_w        = kwargs['max_channel_w']\n",
    "        self.channel_inter_d      = kwargs['channel_inter_d']\n",
    "        self.r_min                = kwargs['r_min']\n",
    "        self.r_pref               = kwargs['r_pref']\n",
    "        self.circle_inter_d       = kwargs['circle_inter_d']\n",
    "        self.circle_spacer        = kwargs['circle_spacer']\n",
    "        self.alter_separation_h   = kwargs['alter_separation_h']\n",
    "        self.h_collapsed_sections = kwargs['h_collapsed_sections']\n",
    "\n",
    "        self.prefilter_dataframe = kwargs['prefilter_dataframe']\n",
    "        self.widget_id           = f'spreadlines_{random.randint(0,65535)}' if kwargs['widget_id'] is None else kwargs['widget_id']\n",
    "        self.w                   = kwargs['w']\n",
    "        self.h                   = kwargs['h']\n",
    "        self.x_ins               = kwargs['x_ins']\n",
    "        self.y_ins               = kwargs['y_ins']\n",
    "        self.txt_h               = kwargs['txt_h']\n",
    "\n",
    "        # Performance information\n",
    "        self.time_lu       = {}\n",
    "        # Unwrap any fields w/ the appropriate transforms\n",
    "        t0 = time.time()\n",
    "        self.__transformFields__()\n",
    "        self.time_lu['transforms'] = time.time() - t0\n",
    "        # Consolidate the fm's and to's into a simple field (__fm0__, __to0__),( __fm1__, __to1__), etc.\n",
    "        t0 = time.time()\n",
    "        self.__consolidateRelationships__()\n",
    "        self.time_lu['consolidate_relationships'] = time.time() - t0\n",
    "\n",
    "        # Prefilter the dataframe (optional... maybe it makes it faster?)\n",
    "        if self.prefilter_dataframe:\n",
    "            t0        = time.time()\n",
    "            _g_       = self.rt_self.createNetworkXGraph(self.df, self.relationships)\n",
    "            self.time_lu['prefilter|create_networkx_graph'] = time.time() - t0\n",
    "            t1        = time.time()\n",
    "            nbors     = set(_g_.neighbors(self.node_focus))\n",
    "            nbors2    = set()\n",
    "            for nbor in nbors: nbors2 = nbors2 | set(_g_.neighbors(nbor))\n",
    "            _to_keep_ = nbors | nbors2 | set([self.node_focus])\n",
    "            self.time_lu['prefilter|networkx_neighbors'] = time.time() - t1\n",
    "            t0        = time.time()\n",
    "            _to_concat_ = []\n",
    "            for _relate_ in self.relationships: # assumes that the relationships are single fields only (via __consolidateRelationships__())\n",
    "                _df_ = self.df.filter(pl.col(_relate_[0]).is_in(_to_keep_) & pl.col(_relate_[1]).is_in(_to_keep_))\n",
    "                _to_concat_.append(_df_)\n",
    "            self.df = pl.concat(_to_concat_)\n",
    "            self.time_lu['prefilter|filter_and_concat_df'] = time.time() - t0\n",
    "\n",
    "        # How many bins?  And what's in those bins for nodes next to the focus?\n",
    "        self.df = self.df.sort(self.ts_field)\n",
    "        _bin_                    = 0\n",
    "        _dfs_containing_focus_   = [] # focus  -> alter1 or alter1 -> focus\n",
    "        _dfs_containing_alter2s_ = [] # alter1 -> alter2 or alter2 -> alter1  ... note does not include focus or alter1 <-> alter1\n",
    "        self.bin_to_timestamps   = {}\n",
    "        self.bin_to_alter1s      = {}\n",
    "        self.bin_to_alter2s      = {}\n",
    "        t0 = time.time()\n",
    "        for k, k_df in self.df.group_by_dynamic(self.ts_field, every=self.every):\n",
    "            _timestamp_     = k[0]\n",
    "            _found_matches_ = False\n",
    "            # find the first alters\n",
    "            for i in range(len(self.relationships)):\n",
    "                _fm_, _to_ = self.relationships[i]\n",
    "                \n",
    "                # From Is Focus\n",
    "                _df_fm_is_focus_ = k_df.filter(pl.col(_fm_) == self.node_focus)\n",
    "                _df_fm_is_focus_ = _df_fm_is_focus_.with_columns(pl.lit(_fm_).alias('__focus_col__'), pl.lit(_to_).alias('__alter_col__'), pl.lit(1).alias('__alter_level__'), pl.lit(_bin_).alias('__bin__'), pl.lit(_timestamp_).alias('__bin_ts__'), pl.lit('to').alias('__alter_side__'))\n",
    "                if len(_df_fm_is_focus_) > 0: \n",
    "                    _dfs_containing_focus_.append(_df_fm_is_focus_)\n",
    "                    if _bin_ not in self.bin_to_alter1s:        self.bin_to_alter1s[_bin_]       = {}\n",
    "                    if 'to'  not in self.bin_to_alter1s[_bin_]: self.bin_to_alter1s[_bin_]['to'] = set()\n",
    "                    self.bin_to_alter1s[_bin_]['to'] |= set(_df_fm_is_focus_[_to_])\n",
    "                    _found_matches_ = True\n",
    "\n",
    "                # To Is Focus\n",
    "                _df_to_is_focus_ = k_df.filter(pl.col(_to_) == self.node_focus)\n",
    "                _df_to_is_focus_ = _df_to_is_focus_.with_columns(pl.lit(_to_).alias('__focus_col__'), pl.lit(_fm_).alias('__alter_col__'), pl.lit(1).alias('__alter_level__'), pl.lit(_bin_).alias('__bin__'), pl.lit(_timestamp_).alias('__bin_ts__'), pl.lit('fm').alias('__alter_side__'))\n",
    "                if len(_df_to_is_focus_) > 0:\n",
    "                    _dfs_containing_focus_.append(_df_to_is_focus_)\n",
    "                    if _bin_ not in self.bin_to_alter1s:        self.bin_to_alter1s[_bin_]       = {}\n",
    "                    if 'fm'  not in self.bin_to_alter1s[_bin_]: self.bin_to_alter1s[_bin_]['fm'] = set()\n",
    "                    self.bin_to_alter1s[_bin_]['fm'] |= set(_df_to_is_focus_[_fm_])\n",
    "                    _found_matches_ = True\n",
    "\n",
    "                # For any shared nodes between the two sides, keep them on the 'fm' side\n",
    "                if _bin_ in self.bin_to_alter1s and 'fm' in self.bin_to_alter1s[_bin_] and 'to' in self.bin_to_alter1s[_bin_]:\n",
    "                    _shared_nodes_ = self.bin_to_alter1s[_bin_]['fm'] & self.bin_to_alter1s[_bin_]['to']\n",
    "                    if len(_shared_nodes_) > 0: self.bin_to_alter1s[_bin_]['to'] -= _shared_nodes_\n",
    "\n",
    "            # find the second alters\n",
    "            if _found_matches_:\n",
    "                _all_alter1s_ = set()\n",
    "                if 'fm' in self.bin_to_alter1s[_bin_]: _all_alter1s_ |= self.bin_to_alter1s[_bin_]['fm']\n",
    "                if 'to' in self.bin_to_alter1s[_bin_]: _all_alter1s_ |= self.bin_to_alter1s[_bin_]['to']\n",
    "                # Go through all the relationships\n",
    "                for i in range(len(self.relationships)):\n",
    "                    _fm_, _to_ = self.relationships[i]\n",
    "                    if 'fm' in self.bin_to_alter1s[_bin_]:\n",
    "                        _df_          = k_df.filter(pl.col(_fm_).is_in(self.bin_to_alter1s[_bin_]['fm']) | pl.col(_to_).is_in(self.bin_to_alter1s[_bin_]['fm']))\n",
    "                        _set_alter2s_ = (set(_df_[_fm_]) | set(_df_[_to_])) - (_all_alter1s_ | set([self.node_focus]))\n",
    "                        if len(_set_alter2s_) > 0:\n",
    "                            if _bin_ not in self.bin_to_alter2s:        self.bin_to_alter2s[_bin_]       = {}\n",
    "                            if 'fm'  not in self.bin_to_alter2s[_bin_]: self.bin_to_alter2s[_bin_]['fm'] = set()\n",
    "                            self.bin_to_alter2s[_bin_]['fm'] |= _set_alter2s_\n",
    "\n",
    "                            _df_ = k_df.filter(pl.col(_fm_).is_in(self.bin_to_alter1s[_bin_]['fm']) & pl.col(_to_).is_in(_set_alter2s_))\n",
    "                            _df_ = _df_.with_columns(pl.lit(_fm_).alias('__alter1_col__'), pl.lit(_to_).alias('__alter2_col__'), pl.lit(2).alias('__alter_level__'), pl.lit(_bin_).alias('__bin__'), pl.lit(_timestamp_).alias('__bin_ts__'), pl.lit('fm').alias('__alter_side__'))\n",
    "                            _dfs_containing_alter2s_.append(_df_)\n",
    "\n",
    "                            _df_ = k_df.filter(pl.col(_to_).is_in(self.bin_to_alter1s[_bin_]['fm']) & pl.col(_fm_).is_in(_set_alter2s_))\n",
    "                            _df_ = _df_.with_columns(pl.lit(_to_).alias('__alter1_col__'), pl.lit(_fm_).alias('__alter2_col__'), pl.lit(2).alias('__alter_level__'), pl.lit(_bin_).alias('__bin__'), pl.lit(_timestamp_).alias('__bin_ts__'), pl.lit('fm').alias('__alter_side__'))\n",
    "                            _dfs_containing_alter2s_.append(_df_)\n",
    "\n",
    "                    if 'to' in self.bin_to_alter1s[_bin_]:\n",
    "                        _df_          = k_df.filter(pl.col(_fm_).is_in(self.bin_to_alter1s[_bin_]['to']) | pl.col(_to_).is_in(self.bin_to_alter1s[_bin_]['to']))\n",
    "                        _set_alter2s_ = (set(_df_[_fm_]) | set(_df_[_to_])) - (_all_alter1s_ | set([self.node_focus]))\n",
    "                        if len(_set_alter2s_) > 0:\n",
    "                            if _bin_ not in self.bin_to_alter2s:        self.bin_to_alter2s[_bin_]       = {}\n",
    "                            if 'to'  not in self.bin_to_alter2s[_bin_]: self.bin_to_alter2s[_bin_]['to'] = set()\n",
    "                            self.bin_to_alter2s[_bin_]['to'] |= _set_alter2s_\n",
    "\n",
    "                            _df_ = k_df.filter(pl.col(_fm_).is_in(self.bin_to_alter1s[_bin_]['to']) & pl.col(_to_).is_in(_set_alter2s_))\n",
    "                            _df_ = _df_.with_columns(pl.lit(_fm_).alias('__alter1_col__'), pl.lit(_to_).alias('__alter2_col__'), pl.lit(2).alias('__alter_level__'), pl.lit(_bin_).alias('__bin__'), pl.lit(_timestamp_).alias('__bin_ts__'), pl.lit('to').alias('__alter_side__'))\n",
    "                            _dfs_containing_alter2s_.append(_df_)\n",
    "\n",
    "                            _df_ = k_df.filter(pl.col(_to_).is_in(self.bin_to_alter1s[_bin_]['to']) & pl.col(_fm_).is_in(_set_alter2s_))\n",
    "                            _df_ = _df_.with_columns(pl.lit(_to_).alias('__alter1_col__'), pl.lit(_fm_).alias('__alter2_col__'), pl.lit(2).alias('__alter_level__'), pl.lit(_bin_).alias('__bin__'), pl.lit(_timestamp_).alias('__bin_ts__'), pl.lit('to').alias('__alter_side__'))\n",
    "                            _dfs_containing_alter2s_.append(_df_)\n",
    "\n",
    "                # For any shared nodes between the two sides, keep them on the 'fm' side\n",
    "                if _bin_ in self.bin_to_alter2s and 'fm' in self.bin_to_alter2s[_bin_] and 'to' in self.bin_to_alter2s[_bin_]:\n",
    "                    _shared_nodes_ = self.bin_to_alter2s[_bin_]['fm'] & self.bin_to_alter2s[_bin_]['to']\n",
    "                    if len(_shared_nodes_) > 0: self.bin_to_alter2s[_bin_]['to'] -= _shared_nodes_\n",
    "\n",
    "            if _found_matches_: \n",
    "                self.bin_to_timestamps[_bin_] = _timestamp_\n",
    "                _bin_ += 1\n",
    "        self.time_lu['alter_binning_step'] = time.time() - t0\n",
    "\n",
    "        # Concatenate the pieces and parts\n",
    "        t0 = time.time()\n",
    "        if len(_dfs_containing_focus_) > 0:   self.df_alter1s = pl.concat(_dfs_containing_focus_).unique()    # unique because we may have duplicate rows on the two sides\n",
    "        else:                                 self.df_alter1s = pl.DataFrame()\n",
    "        if len(_dfs_containing_alter2s_) > 0: self.df_alter2s = pl.concat(_dfs_containing_alter2s_).unique()  # unique because we may have duplicate rows on the two sides\n",
    "        else:                                 self.df_alter2s = pl.DataFrame()\n",
    "        self.time_lu['alter_binning_concat'] = time.time() - t0\n",
    "\n",
    "        self.last_render = None\n",
    "\n",
    "    # nodesInBin() - return the set of nodes that exist in this bin\n",
    "    def nodesInBin(self, bin):\n",
    "        nodes_in_this_bin = set()\n",
    "        if bin in self.bin_to_alter1s and 'fm' in self.bin_to_alter1s[bin]: nodes_in_this_bin |= self.bin_to_alter1s[bin]['fm']\n",
    "        if bin in self.bin_to_alter1s and 'to' in self.bin_to_alter1s[bin]: nodes_in_this_bin |= self.bin_to_alter1s[bin]['to']\n",
    "        if bin in self.bin_to_alter2s and 'fm' in self.bin_to_alter2s[bin]: nodes_in_this_bin |= self.bin_to_alter2s[bin]['fm']\n",
    "        if bin in self.bin_to_alter2s and 'to' in self.bin_to_alter2s[bin]: nodes_in_this_bin |= self.bin_to_alter2s[bin]['to']\n",
    "        return nodes_in_this_bin\n",
    "\n",
    "    # nodesExistInOtherBins() - return the set of nodes that exist in this bin AND'ed with all the other bins\n",
    "    def nodesExistsInOtherBins(self, bin):\n",
    "        nodes_in_this_bin = self.nodesInBin(bin)\n",
    "        all_other_bins    = set()\n",
    "        for _bin_ in (self.bin_to_alter1s.keys()|self.bin_to_alter2s.keys()):\n",
    "            if _bin_ == bin: continue\n",
    "            all_other_bins |= self.nodesInBin( _bin_)\n",
    "        return nodes_in_this_bin & all_other_bins\n",
    "\n",
    "    #\n",
    "    # svgSketch() - produce a basic sketch of how many nodes would occur where in the final rendering...\n",
    "    #\n",
    "    def svgSketch(self):\n",
    "        w_usable, h_usable = self.w - 2*self.x_ins, self.h - 2*self.y_ins\n",
    "        y_mid              = self.y_ins + h_usable/2\n",
    "        bin_to_x           = {}\n",
    "        bin_inter_dist     = w_usable/(len(self.bin_to_alter1s) - 1)\n",
    "        for _bin_ in self.bin_to_alter1s: bin_to_x[_bin_] = self.x_ins + _bin_*bin_inter_dist\n",
    "        _y_diff_alter1s_, _y_diff_alter2s_ = h_usable/8, 2*h_usable/8\n",
    "\n",
    "        svg = [f'<svg x=\"0\" y=\"0\" width=\"{self.w}\" height=\"{self.h}\">']\n",
    "        svg.append(f'<rect x=\"0\" y=\"0\" width=\"{self.w}\" height=\"{self.h}\" fill=\"{self.rt_self.co_mgr.getTVColor(\"background\",\"default\")}\" />')\n",
    "\n",
    "        svg.append(f'<line x1=\"{self.x_ins}\" y1=\"{y_mid}\" x2=\"{self.x_ins+w_usable}\" y2=\"{y_mid}\" stroke=\"{self.rt_self.co_mgr.getTVColor(\"axis\",\"major\")}\" stroke-width=\"4\" />')        \n",
    "        for _bin_ in bin_to_x:\n",
    "            _x_ = bin_to_x[_bin_]\n",
    "            svg.append(f'<line x1=\"{_x_}\" y1=\"{self.y_ins}\" x2=\"{_x_}\" y2=\"{self.y_ins + h_usable}\" stroke=\"{self.rt_self.co_mgr.getTVColor(\"axis\",\"minor\")}\" stroke-width=\"1.0\" />')\n",
    "            svg.append(f'<circle cx=\"{_x_}\" cy=\"{y_mid}\" r=\"5\" stroke=\"{self.rt_self.co_mgr.getTVColor(\"axis\",\"minor\")}\" stroke-width=\"1.0\" fill=\"{self.rt_self.co_mgr.getTVColor('data','default')}\" />')\n",
    "            _date_str_ = self.bin_to_timestamps[_bin_].strftime(self.__dateFormat__())\n",
    "            svg.append(self.rt_self.svgText(_date_str_, _x_-2, self.y_ins + h_usable + 4, rt.co_mgr.getTVColor('axis','minor'), anchor='begin', rotation=270))\n",
    "            if _bin_ in self.bin_to_alter1s and 'fm' in self.bin_to_alter1s[_bin_]: # top of the image\n",
    "                _y_         = y_mid - _y_diff_alter1s_\n",
    "                _num_nodes_ = len(self.bin_to_alter1s[_bin_]['fm'])\n",
    "                svg.append(self.rt_self.svgText(str(_num_nodes_), _x_+2, _y_ + 4, 'black', anchor='begin', rotation=90))\n",
    "                if _bin_ in self.bin_to_alter2s and 'fm' in self.bin_to_alter2s[_bin_]:\n",
    "                    _y_         = y_mid - _y_diff_alter2s_\n",
    "                    _num_nodes_ = len(self.bin_to_alter2s[_bin_]['fm'])\n",
    "                    svg.append(self.rt_self.svgText(str(_num_nodes_), _x_+2, _y_ + 4, 'black', anchor='begin', rotation=90))\n",
    "            if _bin_ in self.bin_to_alter1s and 'to' in self.bin_to_alter1s[_bin_]: # bottom of the image\n",
    "                _y_         = y_mid + _y_diff_alter1s_\n",
    "                _num_nodes_ = len(self.bin_to_alter1s[_bin_]['to'])\n",
    "                svg.append(self.rt_self.svgText(str(_num_nodes_), _x_+2, _y_ + 4, 'black', anchor='begin', rotation=90))\n",
    "                if _bin_ in self.bin_to_alter2s and 'to' in self.bin_to_alter2s[_bin_]:\n",
    "                    _y_         = y_mid + _y_diff_alter2s_\n",
    "                    _num_nodes_ = len(self.bin_to_alter2s[_bin_]['to'])\n",
    "                    svg.append(self.rt_self.svgText(str(_num_nodes_), _x_+2, _y_ + 4, 'black', anchor='begin', rotation=90))\n",
    "\n",
    "        svg.append('</svg>')\n",
    "        return ''.join(svg)\n",
    "\n",
    "    # __dateFormat__() - various date formats based on the value of self.every\n",
    "    def __dateFormat__(self):\n",
    "        if   'd' in self.every: return '%Y-%m-%d'\n",
    "        elif 'h' in self.every: return '%Y-%m-%d %H'\n",
    "        else:                   return '%Y-%m-%d %H:%M'\n",
    "\n",
    "    # packagle() - pack the nodes into the available space\n",
    "    def packable(self, nodes, x, y, y_max, w_max, mul, r_min, r_pref, circle_inter_d, circle_spacer):\n",
    "        node_to_xy = {}\n",
    "        h = abs(y - y_max)\n",
    "        n = len(nodes)\n",
    "        if n > 0:\n",
    "            # single strand\n",
    "            r = ((h - (n-1)*circle_inter_d)/n)/2.0\n",
    "            if r >= r_min:\n",
    "                r          = min(r, r_pref)\n",
    "                left_overs = 0\n",
    "                out_of     = n\n",
    "                for _node_i_ in range(len(nodes)):\n",
    "                    _node_ = nodes[-(_node_i_+1)]\n",
    "                    #if mul == -1: _node_ = nodes[_node_i_]\n",
    "                    #else:         _node_ = nodes[-(_node_i_+1)]\n",
    "                    node_to_xy[_node_] = (x, y+mul*r, r)\n",
    "                    y += mul*(2*r+circle_inter_d)\n",
    "            else:\n",
    "                # m-strands\n",
    "                m_max = w_max / (2*r_min+circle_spacer)\n",
    "                for m in range(2,int(m_max)+1):\n",
    "                    r = (h - (n//m)*circle_inter_d)/(n//m)/2.0\n",
    "                    if r >= r_min:\n",
    "                        r = min(r, r_pref)\n",
    "                        total_width_required = m*(2*r) + (m-1)*circle_spacer\n",
    "                        if total_width_required > w_max: continue\n",
    "                        _col_, nodes_in_this_column = 0, 0\n",
    "                        nodes_per_column = n//m\n",
    "                        left_overs       = n - nodes_per_column*m\n",
    "                        out_of           = nodes_per_column\n",
    "                        if left_overs > 0: m += 1\n",
    "                        total_width_required = m*(2*r) + (m-1)*circle_spacer\n",
    "                        _columns_ = []\n",
    "                        _column_  = []\n",
    "                        for _node_ in nodes:\n",
    "                            _x_col_ = x - total_width_required/2.0 + _col_*(2*r+circle_spacer) + r\n",
    "                            _y_row_ = y+mul*r+mul*nodes_in_this_column*(2*r+circle_inter_d)                        \n",
    "                            _column_.append((_x_col_, _y_row_))\n",
    "                            nodes_in_this_column += 1\n",
    "                            if nodes_in_this_column >= nodes_per_column: \n",
    "                                _columns_.append(_column_)\n",
    "                                _column_  = []\n",
    "                                _col_, nodes_in_this_column = _col_+1, 0\n",
    "                        if len(_column_) > 0: _columns_.append(_column_)\n",
    "                        # Allocate the across first... and then down...\n",
    "                        _xi_, _yi_ = 0, 0\n",
    "                        for _node_i_ in range(len(nodes)):\n",
    "                            if mul == -1: _node_ = nodes[len(nodes) - 1 - _node_i_]\n",
    "                            else:         _node_ = nodes[_node_i_]\n",
    "                            if _yi_ >= len(_columns_[_xi_]): _yi_, _xi_ = _yi_ + 1, 0\n",
    "                            _xy_ = _columns_[_xi_][_yi_]\n",
    "                            node_to_xy[_node_] = (_xy_[0], _xy_[1], r) \n",
    "                            _xi_ += 1\n",
    "                            if _xi_ >= len(_columns_): _yi_, _xi_ = _yi_ + 1, 0\n",
    "                        break\n",
    "\n",
    "        if len(node_to_xy) == 0: return None, None, None\n",
    "        return node_to_xy, left_overs, out_of\n",
    "\n",
    "\n",
    "\n",
    "    #\n",
    "    # renderAlter()  - render an alter / this is just to render the nodes (or clouds) within the alter\n",
    "    # ... the actual shape of the bin & (alters too) is rendered elsewhere\n",
    "    #\n",
    "    def renderAlter(self, nodes, befores, afters, x, y, y_max, w_max, mul=1, r_min=4.0, r_pref=7.0, circle_inter_d=2.0, circle_spacer=3, h_collapsed_sections=16):\n",
    "        # Bounds state & node positioning\n",
    "        xmin, ymin, xmax, ymax = x-r_pref-circle_inter_d, y-r_pref-circle_inter_d, x+r_pref+circle_inter_d, y+r_pref+circle_inter_d\n",
    "        node_to_xyrepstat = {} # node to (x, y, representation, stat) where representation is ['single','cloud'] and state is ['start,'stop','isolated','continuous']\n",
    "        h   = abs(y_max - y)\n",
    "        svg = []\n",
    "        # Determine the state of the node\n",
    "        def nodeState(seen_before, seen_after):\n",
    "            if   seen_before and seen_after: return 'continuous' # node is seen both before and after this bin\n",
    "            elif seen_before:                return 'stopped'    # node was seen before this bin (but not after)\n",
    "            elif seen_after:                 return 'started'    # node seen after this bin (but not before)\n",
    "            else:                            return 'isolated'   # node is only seen in this bin (and no other bin)\n",
    "        # Create the started/stopped triangles for a single node\n",
    "        def svgTriangle(x,y,r,s,d):\n",
    "            nonlocal xmin, ymin, xmax, ymax\n",
    "            p0      = (x+d*(r/2.0), y)\n",
    "            p1      = (x+d*(r+s),   y+r)\n",
    "            p2      = (x+d*(r+s),   y-r)\n",
    "            for _pt_ in [p0,p1,p2]: xmin, ymin, xmax, ymax = min(xmin, _pt_[0]), min(ymin, _pt_[1]), max(xmax, _pt_[0]), max(ymax, _pt_[1])\n",
    "            _path_  = f'M {p0[0]} {p0[1]} L {p1[0]} {p1[1]} L {p2[0]} {p2[1]} Z'\n",
    "            _color_ = '#ff0000' if d == 1 else '#0000ff'\n",
    "            return f'<path d=\"{_path_}\" stroke=\"none\" fill=\"{_color_}\" />'\n",
    "        # Create the started/stopped triangles for the clouds\n",
    "        def svgCloudTriangle(x,y,offset,s,d):\n",
    "            nonlocal xmin, ymin, xmax, ymax\n",
    "            p0      = (x+d*(offset), y)\n",
    "            p1      = (x+d*(offset+s),   y+s)\n",
    "            p2      = (x+d*(offset+s),   y-s)\n",
    "            for _pt_ in [p0,p1,p2]: xmin, ymin, xmax, ymax = min(xmin, _pt_[0]), min(ymin, _pt_[1]), max(xmax, _pt_[0]), max(ymax, _pt_[1])\n",
    "            _path_  = f'M {p0[0]} {p0[1]} L {p1[0]} {p1[1]} L {p2[0]} {p2[1]} Z'\n",
    "            _color_ = '#d3494e' if d == 1 else '#658cbb'\n",
    "            return f'<path d=\"{_path_}\" stroke=\"none\" fill=\"{_color_}\" />'\n",
    "        # Place the nodes onto the canvas\n",
    "        def placeNodeToXYs(n2xy):\n",
    "            nonlocal xmin, ymin, xmax, ymax,svg\n",
    "            for _node_, _xyr_ in n2xy.items():\n",
    "                svg.append(f'<circle cx=\"{_xyr_[0]}\" cy=\"{_xyr_[1]}\" r=\"{_xyr_[2]}\" stroke=\"{self.rt_self.co_mgr.getTVColor(\"axis\",\"major\")}\" stroke-width=\"0.75\" fill=\"none\"/>')\n",
    "                xmin, ymin, xmax, ymax = min(xmin, _xyr_[0]-_xyr_[2]), min(ymin, _xyr_[1]-_xyr_[2]), max(xmax, _xyr_[0]+_xyr_[2]), max(ymax, _xyr_[1]+_xyr_[2])\n",
    "                if _node_ not in befores: svg.append(svgTriangle(_xyr_[0], _xyr_[1], _xyr_[2], circle_spacer/2, -1))\n",
    "                if _node_ not in afters:  svg.append(svgTriangle(_xyr_[0], _xyr_[1], _xyr_[2], circle_spacer/2,  1))\n",
    "                node_to_xyrepstat[_node_] = (_xyr_[0], _xyr_[1], 'single', nodeState(_node_ in befores, _node_ in afters))\n",
    "        # Render the summarization cloud\n",
    "        def summarizationCloud(n, y_cloud, ltriangle, rtriangle, nodes_in_cloud):\n",
    "            nonlocal xmin, ymin, xmax, ymax,svg\n",
    "            svg.append(self.rt_self.iconCloud(x,y_cloud, fg='#e0e0e0', bg='#e0e0e0'))\n",
    "            if ltriangle: svg.append(svgCloudTriangle(x, y_cloud, 16, 6, -1))\n",
    "            if rtriangle: svg.append(svgCloudTriangle(x, y_cloud, 16, 6,  1))\n",
    "            svg.append(self.rt_self.svgText(str(n), x, y_cloud + 4, 'black', anchor='middle'))\n",
    "            xmin, ymin, xmax, ymax = min(xmin, x-16), min(ymin, y_cloud-6), max(xmax, x+16), max(ymax, y_cloud+6)\n",
    "            for _node_ in nodes_in_cloud: node_to_xyrepstat[_node_] = (x, y_cloud, 'cloud', nodeState(not ltriangle, not rtriangle))\n",
    "        # Make sure there are nodes...\n",
    "        if len(nodes) > 0:\n",
    "            # Sort the nodes into the 4 categories\n",
    "            nodes_sorter = []\n",
    "            nodes_isolated, nodes_started, nodes_stopped, nodes_continuous = [], [], [], []\n",
    "            for _node_ in nodes:\n",
    "                if   _node_ in befores and _node_ in afters: nodes_sorter.append((3, _node_)), nodes_continuous.append(_node_)\n",
    "                elif _node_ in befores:                      nodes_sorter.append((2, _node_)), nodes_stopped   .append(_node_)\n",
    "                elif _node_ in afters:                       nodes_sorter.append((1, _node_)), nodes_started   .append(_node_)\n",
    "                else:                                        nodes_sorter.append((0, _node_)), nodes_isolated  .append(_node_)\n",
    "            nodes_sorter  = sorted(nodes_sorter)\n",
    "            \n",
    "            if self.only_render_nodes is not None:\n",
    "                continuous_set, isolated_set, started_set, stopped_set = set(), set(), set(), set()\n",
    "                nodes_ordered = []\n",
    "                for i in range(len(nodes_sorter)):\n",
    "                    _node_ = nodes_sorter[i][1]\n",
    "                    if   _node_ in self.only_render_nodes: nodes_ordered.  append(_node_)\n",
    "                    elif _node_ in nodes_continuous:       continuous_set. add   (_node_)\n",
    "                    elif _node_ in nodes_isolated:         isolated_set.   add   (_node_)\n",
    "                    elif _node_ in nodes_started:          started_set.    add   (_node_)\n",
    "                    elif _node_ in nodes_stopped:          stopped_set.    add   (_node_)\n",
    "                ybase = ymin if mul < 0 else ymax\n",
    "                if len(nodes_ordered) > 0:\n",
    "                    node_to_xy, leftovers, out_of = self.packable(nodes_ordered, x, y, y_max, w_max, mul, r_min, r_pref, circle_inter_d, circle_spacer)\n",
    "                    if node_to_xy is not None:    placeNodeToXYs(node_to_xy) # no summarization necessary\n",
    "                    else:                         summarizationCloud(len(nodes_ordered),  ybase+mul*0.5*h_collapsed_sections, False, False, nodes_ordered)\n",
    "                ybase = ymin if mul < 0 else ymax\n",
    "                if len(nodes_continuous) > 0:     \n",
    "                    summarizationCloud(len(continuous_set), ybase+mul*0.5*h_collapsed_sections, False, False, list(continuous_set))\n",
    "                    ybase = ymin if mul < 0 else ymax\n",
    "                if len(nodes_started)    > 0:     \n",
    "                    summarizationCloud(len(started_set),    ybase+mul*0.5*h_collapsed_sections, True,  False, list(started_set))\n",
    "                    ybase = ymin if mul < 0 else ymax\n",
    "                if len(nodes_stopped)    > 0:     \n",
    "                    summarizationCloud(len(stopped_set),    ybase+mul*0.5*h_collapsed_sections, False, True,  list(stopped_set))\n",
    "                    ybase = ymin if mul < 0 else ymax\n",
    "                if len(nodes_isolated)   > 0:     \n",
    "                    summarizationCloud(len(isolated_set),   ybase+mul*0.5*h_collapsed_sections, True,  True,  list(isolated_set))\n",
    "            else:\n",
    "                # Try putting them all down first... which won't work for any non-trivial number of nodes\n",
    "                nodes_ordered = [x[1] for x in nodes_sorter]\n",
    "                node_to_xy, leftovers, out_of = self.packable(nodes_ordered, x, y, y_max, w_max, mul, r_min, r_pref, circle_inter_d, circle_spacer)\n",
    "                if node_to_xy is not None:\n",
    "                    placeNodeToXYs(node_to_xy) # no summarization necessary\n",
    "                else:\n",
    "                    top_adjust = h_collapsed_sections if mul == 1 else -h_collapsed_sections\n",
    "                    node_to_xy, leftovers, out_of = self.packable(nodes_started+nodes_stopped+nodes_continuous, x, y, y_max-top_adjust, w_max, mul, r_min, r_pref, circle_inter_d, circle_spacer)\n",
    "                    if node_to_xy is not None:\n",
    "                        placeNodeToXYs(node_to_xy) # summarize isolated nodes only\n",
    "                        y_off = ymin if mul == 1 else ymax\n",
    "                        summarizationCloud(len(nodes_isolated), y_off+mul*0.5*h_collapsed_sections, True, True, nodes_isolated)\n",
    "                    else:\n",
    "                        top_adjust = 2*h_collapsed_sections if mul == 1 else -2*h_collapsed_sections\n",
    "                        node_to_xy, leftovers, out_of = self.packable(nodes_started              +nodes_continuous, x, y, y_max-top_adjust, w_max, mul, r_min, r_pref, circle_inter_d, circle_spacer)\n",
    "                        if node_to_xy is not None:\n",
    "                            placeNodeToXYs(node_to_xy) # summarize isolated nodes and nodes_stopped\n",
    "                            y_off = ymax if mul == 1 else ymin\n",
    "                            summarizationCloud(len(nodes_stopped),  y_off+mul*0.5*h_collapsed_sections, False,  True, nodes_stopped)\n",
    "                            summarizationCloud(len(nodes_isolated), y_off+mul*1.5*h_collapsed_sections, True,   True, nodes_isolated)\n",
    "                        else:\n",
    "                            node_to_xy, leftovers, out_of = self.packable(nodes_stopped+nodes_continuous, x, y, y_max-top_adjust, w_max, mul, r_min, r_pref, circle_inter_d, circle_spacer)\n",
    "                            if node_to_xy is not None:\n",
    "                                placeNodeToXYs(node_to_xy) # summarize isolated nodes and nodes_started\n",
    "                                y_off = ymax if mul == 1 else ymin\n",
    "                                summarizationCloud(len(nodes_started),   y_off+mul*0.5*h_collapsed_sections, True,  False, nodes_started)\n",
    "                                summarizationCloud(len(nodes_isolated),  y_off+mul*1.5*h_collapsed_sections, True,  True,  nodes_isolated)\n",
    "                            else:\n",
    "                                top_adjust = 3*h_collapsed_sections if mul == 1 else -3*h_collapsed_sections\n",
    "                                node_to_xy, leftovers, out_of = self.packable(nodes_continuous, x, y, y_max-top_adjust, w_max, mul, r_min, r_pref, circle_inter_d, circle_spacer)\n",
    "                                if node_to_xy is not None:\n",
    "                                    placeNodeToXYs(node_to_xy) # summarize everyting but the continuous nodes (nodes seen in both directions)\n",
    "                                    y_off = ymax if mul == 1 else ymin\n",
    "                                    summarizationCloud(len(nodes_started),   y_off+mul*0.5*h_collapsed_sections, True,  False, nodes_started)\n",
    "                                    summarizationCloud(len(nodes_stopped),   y_off+mul*1.5*h_collapsed_sections, False, True,  nodes_stopped)\n",
    "                                    summarizationCloud(len(nodes_isolated),  y_off+mul*2.5*h_collapsed_sections, True,  True,  nodes_isolated)\n",
    "                                else:\n",
    "                                    # everything is summarized :(\n",
    "                                    summarizationCloud(len(nodes_continuous), y+mul*0.5*h_collapsed_sections, False,  False, nodes_continuous)\n",
    "                                    summarizationCloud(len(nodes_started),    y+mul*1.5*h_collapsed_sections, True,   False, nodes_started)\n",
    "                                    summarizationCloud(len(nodes_stopped),    y+mul*2.5*h_collapsed_sections, False,  True,  nodes_stopped)\n",
    "                                    summarizationCloud(len(nodes_isolated),   y+mul*3.5*h_collapsed_sections, True,   True,  nodes_isolated)\n",
    "            \n",
    "        xmin, ymin, xmax, ymax = xmin - r_pref, ymin - r_pref, xmax + r_pref, ymax + r_pref\n",
    "        # svg.append(f'<rect x=\"{xmin}\" y=\"{ymin}\" width=\"{xmax-xmin}\" height=\"{ymax-ymin}\" stroke=\"{self.rt_self.co_mgr.getTVColor(\"axis\",\"major\")}\" stroke-width=\"0.8\" fill=\"none\" rx=\"{r_pref}\" />')\n",
    "        return ''.join(svg), (xmin, ymin, xmax, ymax), node_to_xyrepstat\n",
    "\n",
    "    # bubbleNumberOnLine() - draw a bubble with a number on a line\n",
    "    def bubbleNumberOnLine(self, x0, x1, y, txt, txt_h=12, color=\"#e0e0e0\", width=3.0):\n",
    "        xm = (x0+x1)/2.0\n",
    "        txt_w = self.rt_self.textLength(txt, txt_h)\n",
    "        x0_1  = xm - 3*txt_w/4.0\n",
    "        x0_2  = x0_1 - txt_w/2\n",
    "        x1_1  = xm + 3*txt_w/4.0\n",
    "        x1_2  = x1_1 + txt_w/2\n",
    "        y_top = y-txt_h/2 - 2\n",
    "        y_bot = y+txt_h/2 + 2\n",
    "        h     = txt_h+4\n",
    "        svg   = []\n",
    "        #svg.append(f'<line x1=\"{x0}\" y1=\"{y}\" x2=\"{x0_1}\" y2=\"{y}\" stroke=\"{color}\" stroke-width=\"{width}\" />')\n",
    "        #svg.append(f'<line x1=\"{x1}\" y1=\"{y}\" x2=\"{x1_1}\" y2=\"{y}\" stroke=\"{color}\" stroke-width=\"{width}\" />')\n",
    "        #svg.append(f'<rect x=\"{x0_1}\" y=\"{y_top}\" width=\"{1.5*txt_w}\" height=\"{h}\" fill=\"{color}\" />')\n",
    "        p = [f'M {x0} {y} L {x0_2} {y}']\n",
    "        p.append(f'C {x0_1} {y} {x0_2} {y_top} {x0_1} {y_top}')\n",
    "        p.append(f'L {x1_1} {y_top}')\n",
    "        p.append(f'C {x1_2} {y_top} {x1_1} {y} {x1_2} {y}')\n",
    "        p.append(f'L {x1} {y} L {x1_2} {y}')\n",
    "        p.append(f'C {x1_1} {y} {x1_2} {y_bot} {x1_1} {y_bot}')\n",
    "        p.append(f'L {x0_1} {y_bot}')\n",
    "        p.append(f'C {x0_2} {y_bot} {x0_1} {y} {x0_2} {y}')\n",
    "        p.append(f'L {x0} {y}')\n",
    "        svg.append(f'<path d=\"{\" \".join(p)}\" stroke=\"{color}\" stroke-width=\"{width}\" fill=\"{color}\"/>')\n",
    "        svg.append(self.rt_self.svgText(txt, xm, y+txt_h/3, txt_h=txt_h, anchor='middle'))\n",
    "        return ''.join(svg)\n",
    "\n",
    "\n",
    "    # svgCrossConnect() - draws a cross connect\n",
    "    def svgCrossConnect(self, x0, y0, x1, y1, launch=None, shift0=None, shift1=None, color=\"#000000\", width=1.0):\n",
    "        if launch is None: launch = (x1-x0)*0.1\n",
    "        if shift0 is None: shift0 = 0\n",
    "        if shift1 is None: shift1 = 0\n",
    "        xmid = (x0+x1)/2\n",
    "        return f'<path d=\"M {x0} {y0} L {x0+launch} {y0} C {xmid+shift0} {y0} {xmid-shift1} {y1} {x1-launch} {y1} L {x1} {y1}\" stroke=\"{color}\" stroke-width=\"{width}\" fill=\"none\" />'\n",
    "\n",
    "    #\n",
    "    # renderBin()\n",
    "    #\n",
    "    def renderBin(self, \n",
    "                  bin,                        # bin index\n",
    "                  x,                          # center of the bin \n",
    "                  y,                          # center of the bin\n",
    "                  max_w,                      # max width of the bin (i.e., the max width of any of the alters)\n",
    "                  max_h):                     # max height of the bin (halfed in each direction from y)\n",
    "        \n",
    "        r_min                = self.r_min \n",
    "        r_pref               = self.r_pref\n",
    "        circle_inter_d       = self.circle_inter_d\n",
    "        circle_spacer        = self.circle_spacer\n",
    "        alter_separation_h   = self.alter_separation_h\n",
    "        h_collapsed_sections = self.h_collapsed_sections\n",
    "\n",
    "        _all_nodes_in_this_bin = self.nodesInBin(bin)\n",
    "        _nodes_in_other_bins_  = self.nodesExistsInOtherBins(bin)\n",
    "        _befores_, _afters_    = set(), set()\n",
    "        for i in range(bin):                                       _befores_ |= self.nodesInBin(i)\n",
    "        for i in range(bin+1, len(self.bin_to_timestamps.keys())): _afters_  |= self.nodesInBin(i)\n",
    "        svg         = [f'<circle cx=\"{x}\" cy=\"{y}\" r=\"{r_pref}\" stroke=\"{self.rt_self.co_mgr.getTVColor(\"axis\",\"minor\")}\" stroke-width=\"0.4\" fill=\"{self.rt_self.co_mgr.getTVColor(\"data\",\"default\")}\" />']\n",
    "        max_alter_h = max_h/5.0\n",
    "\n",
    "        node_2_xyrs = dict()\n",
    "\n",
    "        # Actual alters\n",
    "        if bin in self.bin_to_alter1s and 'fm' in self.bin_to_alter1s[bin]:\n",
    "            _svg_, _bounds_, _n2xyrs_ = self.renderAlter(self.bin_to_alter1s[bin]['fm'], _befores_, _afters_, x, y-r_pref-2*circle_inter_d, y-r_pref-max_alter_h,                  max_w, -1, r_min, r_pref, circle_inter_d, circle_spacer, h_collapsed_sections)\n",
    "            svg.append(_svg_), node_2_xyrs.update(_n2xyrs_)\n",
    "            alter1s_fm_bounds = _bounds_\n",
    "        else:\n",
    "            alter1s_fm_bounds = None\n",
    "            _bounds_          = (x-r_pref, y-r_pref-2*circle_inter_d-5, x+r_pref, y-r_pref-2*circle_inter_d)\n",
    "\n",
    "        if bin in self.bin_to_alter2s and 'fm' in self.bin_to_alter2s[bin]:\n",
    "            _svg_, _bounds_, _n2xyrs_ = self.renderAlter(self.bin_to_alter2s[bin]['fm'], _befores_, _afters_, x, _bounds_[1]-alter_separation_h, _bounds_[1]-alter_separation_h-max_alter_h, max_w, -1, r_min, r_pref, circle_inter_d, circle_spacer, h_collapsed_sections)\n",
    "            svg.append(_svg_), node_2_xyrs.update(_n2xyrs_)\n",
    "            alter2s_fm_bounds = _bounds_\n",
    "        else: alter2s_fm_bounds = None\n",
    "\n",
    "        if bin in self.bin_to_alter1s and 'to' in self.bin_to_alter1s[bin]:\n",
    "            _svg_, _bounds_, _n2xyrs_ = self.renderAlter(self.bin_to_alter1s[bin]['to'], _befores_, _afters_, x, y+r_pref+2*circle_inter_d, y+r_pref+2*circle_inter_d+max_alter_h, max_w,  1, r_min, r_pref, circle_inter_d, circle_spacer, h_collapsed_sections)\n",
    "            svg.append(_svg_), node_2_xyrs.update(_n2xyrs_)\n",
    "            alter1s_to_bounds = _bounds_\n",
    "        else: \n",
    "            _bounds_ = (x-r_pref, y+r_pref+2*circle_inter_d, x+r_pref, y+r_pref+2*circle_inter_d+5)\n",
    "            alter1s_to_bounds = None\n",
    "\n",
    "        if bin in self.bin_to_alter2s and 'to' in self.bin_to_alter2s[bin]:\n",
    "            _svg_, _bounds_, _n2xyrs_ = self.renderAlter(self.bin_to_alter2s[bin]['to'], _befores_, _afters_, x, _bounds_[3]+alter_separation_h, _bounds_[3]+alter_separation_h+max_alter_h, max_w, 1, r_min, r_pref, circle_inter_d, circle_spacer, h_collapsed_sections)\n",
    "            svg.append(_svg_), node_2_xyrs.update(_n2xyrs_)\n",
    "            alter2s_to_bounds = _bounds_\n",
    "        else: alter2s_to_bounds = None\n",
    "\n",
    "        # Calculate the outline of the bin\n",
    "        overall_w = 2*r_pref\n",
    "        if alter1s_fm_bounds is not None: overall_w = max(overall_w, alter1s_fm_bounds[2]-alter1s_fm_bounds[0])\n",
    "        if alter1s_to_bounds is not None: overall_w = max(overall_w, alter1s_to_bounds[2]-alter1s_to_bounds[0])\n",
    "        if alter2s_fm_bounds is not None: overall_w = max(overall_w, alter2s_fm_bounds[2]-alter2s_fm_bounds[0])\n",
    "        if alter2s_to_bounds is not None: overall_w = max(overall_w, alter2s_to_bounds[2]-alter2s_to_bounds[0])\n",
    "        narrow_w = overall_w - 2*r_pref\n",
    "        _amt_    = 2*r_pref\n",
    "        d_array  = [f'M {x-overall_w/2.0} {y}']\n",
    "        if alter1s_to_bounds is None:\n",
    "            d_array.append(f'L {x-overall_w/2.0} {y+  _amt_}  C {x-overall_w/2.0} {y+2*_amt_} {x-overall_w/2.0} {y+2*_amt_} {x-narrow_w/2.0}  {y+2*_amt_}')\n",
    "            d_array.append(f'L {x+narrow_w/2.0}  {y+2*_amt_}  C {x+overall_w/2.0} {y+2*_amt_} {x+overall_w/2.0} {y+2*_amt_} {x+overall_w/2.0} {y+  _amt_}')\n",
    "            d_array.append(f'L {x+overall_w/2.0} {y}')\n",
    "        elif alter2s_to_bounds is None:\n",
    "            ah = alter1s_to_bounds[3]-alter1s_to_bounds[1]-2*_amt_\n",
    "            d_array.append(f'L {x-overall_w/2.0} {y+ah+  _amt_}  C {x-overall_w/2.0} {y+ah+2*_amt_} {x-overall_w/2.0} {y+ah+2*_amt_} {x-narrow_w/2.0}  {y+ah+2*_amt_}')\n",
    "            d_array.append(f'L {x+narrow_w/2.0}  {y+ah+2*_amt_}  C {x+overall_w/2.0} {y+ah+2*_amt_} {x+overall_w/2.0} {y+ah+2*_amt_} {x+overall_w/2.0} {y+ah+  _amt_}')\n",
    "            d_array.append(f'L {x+overall_w/2.0} {y}')\n",
    "        else:\n",
    "            ah  = alter1s_to_bounds[3]-alter1s_to_bounds[1]-2*_amt_\n",
    "            d_array.append(f'L {x-overall_w/2.0} {y+ah+  _amt_}  C {x-overall_w/2.0} {y+ah+2*_amt_} {x-overall_w/2.0} {y+ah+2*_amt_} {x-narrow_w/2.0}  {y+ah+2*_amt_}')\n",
    "            a2y  = alter2s_to_bounds[1] + 2*r_pref\n",
    "            d_array.append(f'L {x-narrow_w/2.0}  {a2y}           C {x-overall_w/2.0} {a2y}          {x-overall_w/2.0} {a2y}          {x-overall_w/2.0} {a2y+2*_amt_}')\n",
    "            a2y2 = alter2s_to_bounds[3] - 2*_amt_\n",
    "            d_array.append(f'L {x-overall_w/2.0} {a2y2}')\n",
    "            d_array.append(f'C {x-overall_w/2.0} {a2y2+2*_amt_} {x-overall_w/2.0} {a2y2+2*_amt_} {x-narrow_w/2.0}  {a2y2+2*_amt_}')\n",
    "            d_array.append(f'L {x+narrow_w/2.0}  {a2y2+2*_amt_}  C {x+overall_w/2.0} {a2y2+2*_amt_} {x+overall_w/2.0} {a2y2+2*_amt_} {x+overall_w/2.0} {a2y2}')\n",
    "            d_array.append(f'L {x+overall_w/2.0} {a2y +2*_amt_}  C {x+overall_w/2.0} {a2y}          {x+overall_w/2.0} {a2y}          {x+narrow_w/2.0}  {a2y}')\n",
    "            d_array.append(f'L {x+narrow_w/2.0}  {y+ah+2*_amt_}  C {x+overall_w/2.0} {y+ah+2*_amt_} {x+overall_w/2.0} {y+ah+2*_amt_} {x+overall_w/2.0} {y+ah+  _amt_}')\n",
    "            d_array.append(f'L {x+overall_w/2.0} {y}')\n",
    "\n",
    "        if alter1s_fm_bounds is None:\n",
    "            d_array.append(f'L {x+overall_w/2.0} {y-  _amt_}  C {x+overall_w/2.0} {y-2*_amt_} {x+overall_w/2.0} {y-2*_amt_} {x+narrow_w/2.0}  {y-2*_amt_}')\n",
    "            d_array.append(f'L {x-narrow_w/2.0}  {y-2*_amt_}  C {x-overall_w/2.0} {y-2*_amt_} {x-overall_w/2.0} {y-2*_amt_} {x-overall_w/2.0} {y-  _amt_}')\n",
    "            d_array.append(f'L {x-overall_w/2.0} {y}')\n",
    "        elif alter2s_fm_bounds is None:\n",
    "            ah = alter1s_fm_bounds[3]-alter1s_fm_bounds[1]-2*_amt_\n",
    "            d_array.append(f'L {x+overall_w/2.0} {y-ah-  _amt_}  C {x+overall_w/2.0} {y-ah-2*_amt_} {x+overall_w/2.0} {y-ah-2*_amt_} {x+narrow_w/2.0}  {y-ah-2*_amt_}')\n",
    "            d_array.append(f'L {x-narrow_w/2.0}  {y-ah-2*_amt_}  C {x-overall_w/2.0} {y-ah-2*_amt_} {x-overall_w/2.0} {y-ah-2*_amt_} {x-overall_w/2.0} {y-ah-  _amt_}')\n",
    "            d_array.append(f'L {x-overall_w/2.0} {y}')\n",
    "        else:\n",
    "            ah  = alter1s_fm_bounds[3]-alter1s_fm_bounds[1]-2*_amt_\n",
    "            d_array.append(f'L {x+overall_w/2.0} {y-ah-  _amt_}  C {x+overall_w/2.0} {y-ah-2*_amt_} {x+overall_w/2.0} {y-ah-2*_amt_} {x+narrow_w/2.0}  {y-ah-2*_amt_}')\n",
    "            a2y  = alter2s_fm_bounds[3] - 2*r_pref\n",
    "            d_array.append(f'L {x+narrow_w/2.0}  {a2y}           C {x+overall_w/2.0} {a2y}          {x+overall_w/2.0} {a2y}          {x+overall_w/2.0} {a2y-2*_amt_}')\n",
    "            a2y2 = alter2s_fm_bounds[1] + 2*_amt_\n",
    "            d_array.append(f'L {x+overall_w/2.0} {a2y2}')\n",
    "            d_array.append(f'C {x+overall_w/2.0} {a2y2-2*_amt_} {x+overall_w/2.0} {a2y2-2*_amt_} {x+narrow_w/2.0}  {a2y2-2*_amt_}')\n",
    "            d_array.append(f'L {x-narrow_w/2.0}  {a2y2-2*_amt_}  C {x-overall_w/2.0} {a2y2-2*_amt_} {x-overall_w/2.0} {a2y2-2*_amt_} {x-overall_w/2.0} {a2y2}')\n",
    "            d_array.append(f'L {x-overall_w/2.0} {a2y -2*_amt_}  C {x-overall_w/2.0} {a2y}          {x-overall_w/2.0} {a2y}          {x-narrow_w/2.0}  {a2y}')\n",
    "            d_array.append(f'L {x-narrow_w/2.0}  {y-ah-2*_amt_}  C {x-overall_w/2.0} {y-ah-2*_amt_} {x-overall_w/2.0} {y-ah-2*_amt_} {x-overall_w/2.0} {y-ah-  _amt_}')\n",
    "            d_array.append(f'L {x-overall_w/2.0} {y}')\n",
    "\n",
    "        path_description = \" \".join(d_array)\n",
    "        def pathBounds(s):\n",
    "            x0, y0, x1, y1 = 1e10, 1e10, -1e10, -1e10\n",
    "            ps = s.split()\n",
    "            i  = 0\n",
    "            while i < len(ps):\n",
    "                if ps[i] == \"M\":\n",
    "                    xm, ym = float(ps[i+1]), float(ps[i+2])\n",
    "                    x0, y0, x1, y1 = min(x0,xm), min(y0,ym), max(x1,xm), max(y1,ym)\n",
    "                    i += 3\n",
    "                elif ps[i] == \"L\":\n",
    "                    xl, yl = float(ps[i+1]), float(ps[i+2])\n",
    "                    x0, y0, x1, y1 = min(x0,xl), min(y0,yl), max(x1,xl), max(y1,yl)\n",
    "                    i += 3\n",
    "                elif ps[i] == \"C\":\n",
    "                    cx0, cy0 = float(ps[i+1]), float(ps[i+2])\n",
    "                    cx1, cy1 = float(ps[i+3]), float(ps[i+4])\n",
    "                    xc,  yc  = float(ps[i+5]), float(ps[i+6])\n",
    "                    x0, y0, x1, y1 = min(x0,xc), min(y0,yc), max(x1,xc), max(y1,yc)\n",
    "                    i += 7\n",
    "                else: raise Exception(f\"Unknown command {ps[i]}\")\n",
    "            return x0, y0, x1, y1\n",
    "\n",
    "        svg.append(f'<path d=\"{path_description}\" stroke=\"{self.rt_self.co_mgr.getTVColor(\"axis\",\"major\")}\" stroke-width=\"2.0\" fill=\"none\" />')\n",
    "\n",
    "        return ''.join(svg), pathBounds(path_description), node_2_xyrs\n",
    "\n",
    "    #\n",
    "    # renderSVG()\n",
    "    #\n",
    "    def renderSVG(self):\n",
    "        vx0, vy0, vx1, vy1 = None, None, None, None\n",
    "        svg = []\n",
    "\n",
    "        alter_inter_d   = self.alter_inter_d\n",
    "        max_bin_w       = self.max_bin_w\n",
    "        max_bin_h       = self.max_bin_h\n",
    "        min_channel_w   = self.min_channel_w\n",
    "        max_channel_w   = self.max_channel_w\n",
    "        channel_inter_d = self.channel_inter_d\n",
    "\n",
    "        # Bin Creation\n",
    "        _bins_ordered_ = list(self.bin_to_timestamps.keys())\n",
    "        _bins_ordered_.sort()\n",
    "        bin_to_bounds  = {}\n",
    "        bin_to_n2xyrs  = {}\n",
    "        x, y = alter_inter_d, (self.h-max_bin_h)/2 + max_bin_h/2\n",
    "        for _bin_ in _bins_ordered_:\n",
    "            _svg_, _bounds_, _n2xyrs_ = self.renderBin(_bin_, x, y, max_bin_w, max_bin_h)\n",
    "            bin_to_n2xyrs[_bin_] = _n2xyrs_\n",
    "            bin_to_bounds[_bin_] = _bounds_\n",
    "            svg.append(_svg_)\n",
    "            xmin, ymin, xmax, ymax = _bounds_\n",
    "            x = xmax + alter_inter_d\n",
    "            if vx0 is None: vx0, vy0, vx1, vy1 = _bounds_[0], _bounds_[1], _bounds_[2], _bounds_[3]\n",
    "            vx0, vy0, vx1, vy1 = min(vx0, _bounds_[0]-alter_inter_d/3.0), min(vy0, _bounds_[1] - 3*channel_inter_d), max(vx1, _bounds_[2]+alter_inter_d/3.0), max(vy1, _bounds_[3]+3*channel_inter_d)\n",
    "\n",
    "        # Determine if two bounds overlap - used to separate channels (prevent overlap between channels)\n",
    "        def boundsOverlap(a,b): return a[0] < b[0]+b[2] and a[0]+a[2] > b[0] and a[1] < b[1]+b[3] and a[1]+a[3] > b[1]\n",
    "\n",
    "        # Channel Allocation\n",
    "        bin_to_nodes_to_channel                    = {}\n",
    "        max_nodes_to_channel, min_nodes_to_channel = 0, 1e10\n",
    "        tuple_to_channel_geometry                  = {}\n",
    "        channel_tuples                             = []\n",
    "\n",
    "        for _fm_to_ in ['to','fm']:\n",
    "            for i in range(len(_bins_ordered_)-1, 1, -1):\n",
    "                _bin_   = _bins_ordered_[i]\n",
    "                _nodes_ = set()                                                                    # Get all the nodes in the \"fm\" side of this bin\n",
    "                if _bin_ in self.bin_to_alter1s and _fm_to_ in self.bin_to_alter1s[_bin_]: _nodes_ |= self.bin_to_alter1s[_bin_][_fm_to_]\n",
    "                if _bin_ in self.bin_to_alter2s and _fm_to_ in self.bin_to_alter2s[_bin_]: _nodes_ |= self.bin_to_alter2s[_bin_][_fm_to_]\n",
    "\n",
    "                _nodes_  = _nodes_ - self.nodesInBin(_bins_ordered_[i-1])                             # These will be direct connects / so don't need to channel them\n",
    "\n",
    "                if _fm_to_ == 'fm': y_clearance = bin_to_bounds[_bins_ordered_[i-1]][1] - max_channel_w - channel_inter_d # The channel has to clear this height (this is at the \"top\")\n",
    "                else:               y_clearance = bin_to_bounds[_bins_ordered_[i-1]][3] + max_channel_w + channel_inter_d # The channel has to clear this height (this is at the \"bottom\")\n",
    "\n",
    "                _befores_ = set()                                                                         # All the nodes before this bin\n",
    "                for j in range(i): _befores_ |= self.nodesInBin(_bins_ordered_[j])\n",
    "                _nodes_                         = _nodes_ & _befores_                                     # These are now all the nodes that we need to channel...\n",
    "                number_of_nodes_in_this_channel = len(_nodes_)\n",
    "                max_nodes_to_channel, min_nodes_to_channel = max(len(_nodes_), max_nodes_to_channel), min(len(_nodes_), min_nodes_to_channel)\n",
    "\n",
    "                if len(_nodes_) > 0:                                                                               # If there are any nodes to channel...\n",
    "                    _saving_for_later_ = []\n",
    "                    for j in range(i-2, -1, -1):\n",
    "                        _here_       = _bins_ordered_[j]\n",
    "                        _here_nodes_ = self.nodesInBin(_here_)\n",
    "                        if len(_nodes_ & _here_nodes_) > 0: \n",
    "                            for _node_ in _nodes_ & _here_nodes_: _saving_for_later_.append((_bin_, _here_, _node_))\n",
    "                        _nodes_ = _nodes_ - _here_nodes_                                      \n",
    "                        if len(_nodes_) == 0: break                                                                                    # If there are no more nodes to channel, we're done\n",
    "                        if _fm_to_ == 'fm': y_clearance = min(y_clearance, bin_to_bounds[_here_][1] - max_channel_w - channel_inter_d) # Otherwise, we have to clear this height\n",
    "                        else:               y_clearance = max(y_clearance, bin_to_bounds[_here_][3] + max_channel_w + channel_inter_d) # Otherwise, we have to clear this height\n",
    "                    _channel_tuple_ = (_here_, _bin_, y_clearance, number_of_nodes_in_this_channel, _fm_to_)                           # start bin -> end bin, y_clearance, number of nodes, fm-to side\n",
    "                    channel_tuples.append(_channel_tuple_)                                                                             # will determine the actual geometry later\n",
    "                    for _saved_ in _saving_for_later_:\n",
    "                        _to_bin_, _fm_bin_, _node_ = _saved_\n",
    "                        if _fm_bin_ not in bin_to_nodes_to_channel:           bin_to_nodes_to_channel[_fm_bin_]           = {}\n",
    "                        if _to_bin_ not in bin_to_nodes_to_channel[_fm_bin_]: bin_to_nodes_to_channel[_fm_bin_][_to_bin_] = {}\n",
    "                        if _node_ in bin_to_nodes_to_channel[_fm_bin_][_to_bin_]: raise Exception(f'Duplicate node {_node_} in bin {_bin_} -> {_to_bin_}')\n",
    "                        bin_to_nodes_to_channel[_fm_bin_][_to_bin_][_node_]                                               = _channel_tuple_\n",
    "            \n",
    "        # Sort the channels & render the channels\n",
    "        channel_tuples.sort(key=lambda x: x[2]) # slightly non-optimal... because the two sides (fm, to) should be sorted in opposite directions\n",
    "        for i in range(len(channel_tuples)):\n",
    "            _start_, _end_, _y_, _n_, _fm_to_ = channel_tuples[i]\n",
    "            _div_                             = (max_nodes_to_channel - min_nodes_to_channel)\n",
    "            if _div_ == 0:  _h_               = min_channel_w\n",
    "            else:           _h_               = (_n_ - min_nodes_to_channel)/_div_ * (max_channel_w - min_channel_w) + min_channel_w\n",
    "            _w_                               = bin_to_bounds[_end_][0] - bin_to_bounds[_start_][2] - 1.5*alter_inter_d\n",
    "            _x_                               = bin_to_bounds[_start_][2] + alter_inter_d\n",
    "\n",
    "            placement_okay = False\n",
    "            while placement_okay == False:\n",
    "                placement_okay = True\n",
    "                for _other_ in tuple_to_channel_geometry:\n",
    "                    _geom_ = tuple_to_channel_geometry[_other_]\n",
    "                    if boundsOverlap((_geom_[0], _geom_[1]-channel_inter_d, _geom_[2], _geom_[3]+2*channel_inter_d), (_x_, _y_-channel_inter_d, _w_, _h_+2*channel_inter_d)):\n",
    "                        placement_okay = False\n",
    "                        break\n",
    "                if not placement_okay: \n",
    "                    if _fm_to_ == 'fm':  _y_ -= channel_inter_d\n",
    "                    else:                _y_ += channel_inter_d\n",
    "            \n",
    "            vy0 = min(vy0, _y_       - 3*channel_inter_d)\n",
    "            vy1 = max(vy1, _y_ + _h_ + 3*channel_inter_d)\n",
    "\n",
    "            tuple_to_channel_geometry[channel_tuples[i]] = (_x_, _y_, _w_, _h_)\n",
    "            svg.append(self.bubbleNumberOnLine(_x_, _x_ + _w_, _y_ + _h_/2.0, str(_n_), txt_h=12, color=self.rt_self.co_mgr.getTVColor('axis','major'), width=2.0))\n",
    "\n",
    "        # Draw the direct connects & the channel connections\n",
    "        for i in range(len(_bins_ordered_)-1):\n",
    "            _bin0_     = _bins_ordered_[i]\n",
    "            _bin1_     = _bins_ordered_[i+1]\n",
    "            _bounds0_  = bin_to_bounds[_bin0_]\n",
    "            _bounds1_  = bin_to_bounds[_bin1_]\n",
    "\n",
    "            _already_drawn_ = set()\n",
    "\n",
    "            # direct connects\n",
    "            _nodes_dc_ = bin_to_n2xyrs[_bin0_].keys() & bin_to_n2xyrs[_bin1_].keys()\n",
    "            for _node_ in _nodes_dc_:\n",
    "                _x0_, _y0_, _r0_, _s0_ = bin_to_n2xyrs[_bin0_][_node_]\n",
    "                _x1_, _y1_, _r1_, _s1_ = bin_to_n2xyrs[_bin1_][_node_]\n",
    "                _coords_ = (_bounds0_[2], _y0_, _bounds1_[0], _y1_)\n",
    "                if _coords_ not in _already_drawn_:\n",
    "                    # Color options // still need to do \"vary\"\n",
    "                    if   self.node_color is None:                                      _color_ = self.rt_self.co_mgr.getTVColor('axis','major')\n",
    "                    elif self.node_color == 'node':                                    _color_ = self.rt_self.co_mgr.getColor(_node_)\n",
    "                    elif type(self.node_color) is dict and _node_ in self.node_color:  _color_ = self.rt_self.getColor(self.node_color[_node_])\n",
    "                    else:                                                              _color_ = self.rt_self.co_mgr.getTVColor('axis','major')\n",
    "                    # Render the direct connection & records that it has been rendered -- may prevent node_color == 'vary' from rendering correctly\n",
    "                    svg.insert(0, self.svgCrossConnect(_bounds0_[2], _y0_, _bounds1_[0], _y1_, color=_color_, width=1.5))\n",
    "                    _already_drawn_.add(_coords_)\n",
    "            \n",
    "            # channel connections\n",
    "            for _fm_to_ in ['fm','to']: # different alter sides\n",
    "                if _bin0_ in bin_to_nodes_to_channel: # across the bins\n",
    "                    for _bin_n_ in bin_to_nodes_to_channel[_bin0_]: # which bins does _bin0_ connect to?\n",
    "                        for _node_ in bin_to_nodes_to_channel[_bin0_][_bin_n_]: # pick up those nodes\n",
    "                            _xyrs_             = bin_to_n2xyrs[_bin0_][_node_]\n",
    "                            _channel_tuple_    = bin_to_nodes_to_channel[_bin0_][_bin_n_][_node_]\n",
    "                            _channel_geometry_ = tuple_to_channel_geometry[_channel_tuple_]\n",
    "                            _halfway_          = _bounds1_[0]\n",
    "                            if _halfway_ < _channel_geometry_[0]: _halfway_ = _channel_geometry_[0]\n",
    "                            _channel_vmiddle_  = _channel_geometry_[1] + _channel_geometry_[3]/2.0\n",
    "                            _coords_           = (_bounds0_[2], _xyrs_[1], _channel_geometry_[0], _channel_vmiddle_)\n",
    "                            if _coords_ not in _already_drawn_:\n",
    "                                svg.insert(0, self.svgCrossConnect(_bounds0_[2], _xyrs_[1], _halfway_, _channel_vmiddle_, color=self.rt_self.co_mgr.getTVColor('axis','major'), width=2.0))\n",
    "                                _already_drawn_.add(_coords_)\n",
    "                            _xyrs_endpt_       = bin_to_n2xyrs[_bin_n_][_node_]\n",
    "                            _boundsn_          = bin_to_bounds[_bin_n_]\n",
    "                            _boundsn_minus_1_  = bin_to_bounds[_bin_n_-1]\n",
    "                            _halfway_          = (_boundsn_minus_1_[2] + _boundsn_[0])/2.0\n",
    "                            _coords_           = (_boundsn_[0], _xyrs_endpt_[1], _channel_geometry_[0] + _channel_geometry_[2], _channel_vmiddle_)\n",
    "                            if _coords_ not in _already_drawn_:\n",
    "                                svg.insert(0, self.svgCrossConnect(_boundsn_[0], _xyrs_endpt_[1], _channel_geometry_[0] + _channel_geometry_[2], _channel_vmiddle_, color=self.rt_self.co_mgr.getTVColor('axis','major'), width=2.0))\n",
    "                                _already_drawn_.add(_coords_)\n",
    "\n",
    "        # Add the header and the footer\n",
    "        svg.insert(0, f'<svg x=\"0\" y=\"0\" width=\"{self.w}\" height=\"{self.h}\" viewBox=\"{vx0} {vy0} {vx1-vx0} {vy1-vy0}\">')\n",
    "        svg.insert(1, f'<rect x=\"{vx0}\" y=\"{vy0}\" width=\"{vx1-vx0}\" height=\"{vy1-vy0}\" fill=\"{self.rt_self.co_mgr.getTVColor(\"background\",\"default\")}\" />')\n",
    "        svg.insert(2, f'<line x1=\"{alter_inter_d}\" y1=\"{y}\" x2=\"{x-alter_inter_d - (xmax-xmin)/2}\" y2=\"{y}\" stroke=\"{self.rt_self.co_mgr.getTVColor(\"axis\",\"major\")}\" stroke-width=\"3.0\" />')\n",
    "        svg.append('</svg>')\n",
    "        self.last_render = ''.join(svg)\n",
    "        return self.last_render\n",
    "\n",
    "    #\n",
    "    # SVG Representation Renderer\n",
    "    #\n",
    "    def _repr_svg_(self):\n",
    "        if self.last_render is None: self.renderSVG()\n",
    "        return self.last_render\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrates various configurations of a single alter render\n",
    "'''\n",
    "_tiles_ = []\n",
    "for _num_of_nodes_ in range(500, 1500, 300):\n",
    "    _hdr_ = f'<svg x=\"0\" y=\"0\" width=\"{384}\" height=\"{384}\">'\n",
    "    _bg_  = f'<rect x=\"0\" y=\"0\" width=\"{384}\" height=\"{384}\" fill=\"{rt.co_mgr.getTVColor(\"background\",\"default\")}\" />'\n",
    "    _ftr_ = '</svg>'\n",
    "    _nodes_, _befores_, _afters_ = set(), set(), set()\n",
    "    for i in range(_num_of_nodes_):\n",
    "        _nodes_.add(i)\n",
    "        if random.random() < 0.3: _befores_.add(i)\n",
    "        if random.random() < 0.3: _afters_.add(i)\n",
    "    _svg_, _bounds_, _n2xyrs_ = sl.renderAlter(_nodes_, _befores_, _afters_, 175, 300, 200, 128, mul=-1)\n",
    "    xmin, ymin, xmax, ymax = _bounds_\n",
    "    _box_ = f'<rect x=\"{xmin}\" y=\"{ymin}\" width=\"{xmax-xmin}\" height=\"{ymax-ymin}\" stroke=\"{rt.co_mgr.getTVColor(\"axis\",\"major\")}\" stroke-width=\"0.8\" fill=\"none\" rx=\"10\" />'\n",
    "    _tiles_.append(_hdr_ + _bg_ + _svg_ + _box_ + _ftr_)\n",
    "    _svg_, _bounds_, _n2xyrs_ = sl.renderAlter(_nodes_, _befores_, _afters_, 175, 100, 300,  64, mul= 1)\n",
    "    xmin, ymin, xmax, ymax = _bounds_\n",
    "    _box_ = f'<rect x=\"{xmin}\" y=\"{ymin}\" width=\"{xmax-xmin}\" height=\"{ymax-ymin}\" stroke=\"{rt.co_mgr.getTVColor(\"axis\",\"major\")}\" stroke-width=\"0.8\" fill=\"none\" rx=\"10\" />'\n",
    "    _tiles_.append(_hdr_ + _bg_ + _svg_ + _box_ + _ftr_)\n",
    "#rt.table(_tiles_, per_row=4, spacer=10)'\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#sl = None\n",
    "if 'sl' not in globals() or sl is None:\n",
    "    if use_real_data: sl = spreadLines(rt, df, [('sip','dip')], '172.30.0.4', only_render_nodes=set(['10.156.165.212', '10.16.5.15',]), every=\"2d\", h=384)\n",
    "    else:             sl = spreadLines(rt, df, [('sip','dip')], 'a',          every='1d', h=384)\n",
    "_svg_        = sl.renderSVG()\n",
    "rt.tile([_svg_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'rt_self':rt, 'df':df, 'relationships':[('sip','dip')], 'node_color':'node', 'every':'2d', 'h':384}\n",
    "\n",
    "rt.tile([spreadLines(node_focus='172.30.0.4',                                                           **params),\n",
    "         spreadLines(node_focus='172.30.0.4', only_render_nodes=set(['10.156.165.212', '10.16.5.15',]), **params),\n",
    "         spreadLines(node_focus='172.30.0.4', only_render_nodes=set(),                                  **params)], horz=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
