{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App logic\n",
    "\n",
    "> Contains BaseChatApp and large language model integration logic. Implements the core functionality indepent of the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Protocol, runtime_checkable, Generator, List\n",
    "from openai import OpenAI\n",
    "\n",
    "from gradiochat.config import ModelConfig, Message, ChatAppConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import statement\n",
    "\n",
    "```python\n",
    "from gradiochat.app import *\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of new code\n",
    "\n",
    "I used the `Protocol` and `runtime_checkable` for the first time. Here's a short explainer.\n",
    "\n",
    "#### Python's Protocol System\n",
    "\n",
    "Protocols were introduced in Python 3.8 through PEP 544 and are part of the `typing` module. They provide a way to define interfaces that classes can implement without explicitly inheriting from them - this is called \"structural typing\" or \"duck typing.\"\n",
    "\n",
    "#### Protocols vs Abstract Base Classes (ABCs)\n",
    "\n",
    "**Abstract Base Classes (Traditional Approach):**\n",
    "- Require explicit inheritance (`class MyClass(AbstractBaseClass):`)\n",
    "- Use the `@abstractmethod` decorator to mark methods that must be implemented\n",
    "- Check for compatibility based on the class hierarchy (nominal typing)\n",
    "- Enforce implementation at class definition time\n",
    "\n",
    "**Protocols (New Approach):**\n",
    "- Don't require inheritance - classes just need to implement the required methods\n",
    "- Use the `Protocol` class and `@runtime_checkable` decorator\n",
    "- Check for compatibility based on method signatures (structural typing)\n",
    "- Can check compatibility at runtime with `isinstance()` if marked as `@runtime_checkable`\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "In our code:\n",
    "\n",
    "```python\n",
    "@runtime_checkable\n",
    "class LLMClientProtocol(Protocol):\n",
    "    def chat_completion(self, messages: List[Message], **kwargs) -> str:\n",
    "        ...\n",
    "    \n",
    "    def chat_completion_stream(self, messages: List[Message], **kwargs) -> Generator[str, None, None]:\n",
    "        ...\n",
    "```\n",
    "\n",
    "This defines an interface that says \"any class with methods named `chat_completion` and `chat_completion_stream` with these signatures is considered compatible with `LLMClientProtocol`.\"\n",
    "\n",
    "The `...` in the method bodies is a special syntax that means \"this method is required but not implemented here.\" It's similar to `pass` but specifically for protocol definitions.\n",
    "\n",
    "#### Benefits in Our Context\n",
    "\n",
    "1. **Flexibility**: We can create any class that implements these methods, and it will be compatible with `LLMClientProtocol` without explicitly inheriting from it.\n",
    "\n",
    "2. **Easy Testing**: We can create mock implementations that automatically satisfy the protocol by just implementing the required methods.\n",
    "\n",
    "3. **Type Checking**: Tools like mypy can verify that our classes implement all required methods with the correct signatures.\n",
    "\n",
    "4. **Runtime Checking**: With `@runtime_checkable`, we can use `isinstance(obj, LLMClientProtocol)` to check if an object implements the protocol.\n",
    "\n",
    "#### Example of Use\n",
    "\n",
    "```python\n",
    "def process_with_any_llm_client(client: LLMClientProtocol, messages: List[Message]):\n",
    "    # This function will accept any object that has the required methods,\n",
    "    # regardless of its class hierarchy\n",
    "    response = client.chat_completion(messages)\n",
    "    return response\n",
    "```\n",
    "\n",
    "This would accept our `HuggingFaceClient` or any other class that implements the required methods, without forcing them to inherit from a common base class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the general LLMClientProtocol structure\n",
    "\n",
    "Which means it should have the methods defined in `LLMClientProtocol`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@runtime_checkable\n",
    "class LLMClientProtocol(Protocol):\n",
    "    \"\"\"Protocol defining the interface for LLM clients\"\"\"\n",
    "    \n",
    "    def chat_completion(self, messages: List[Message], **kwargs) -> str:\n",
    "        \"\"\"Generate a response from the LLM\"\"\"\n",
    "        ...\n",
    "    \n",
    "    def chat_completion_stream(self, messages: List[Message], **kwargs) -> Generator[str, None, None]:\n",
    "        \"\"\"Generate a streaming response from the LLM\"\"\"\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the HuggingFaceClient\n",
    "\n",
    "This should at least follow the structure of `LLMClientProtocol` but can of course be expanded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HuggingFaceClient():\n",
    "    \"\"\"Client for interacting with HuggingFace models\"\"\"\n",
    "    \n",
    "    def __init__(self, model_config: ModelConfig):\n",
    "        \"\"\"Initialize the client with model configuration\"\"\"\n",
    "        self.model_config = model_config\n",
    "\n",
    "        # Default to HF Inference API if no base URL is provided\n",
    "        base_url = model_config.api_base_url or \"https://router.huggingface.co/hf-inference/v1\"\n",
    "\n",
    "        self.client = OpenAI(\n",
    "            base_url=base_url,\n",
    "            api_key=model_config.api_key or \"hf_no_api_key_provided\"\n",
    "        )\n",
    "    \n",
    "    def chat_completion(self, \n",
    "            messages: List[Message], # List of messages conforming to the Message pydantic dataclass\n",
    "            **kwargs\n",
    "            ) -> str:\n",
    "        \"\"\"Generate a chat completion from the HuggingFace model\"\"\"\n",
    "        # Convert our Message objects to the format expected by the OpenAI client\n",
    "        openai_messages = [{\"role\": msg.role, \"content\": msg.content} for msg in messages]\n",
    "\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=self.model_config.model_name,\n",
    "            messages=openai_messages,\n",
    "            max_tokens=kwargs.get(\"max_tokens\", self.model_config.max_tokens),\n",
    "            temperature=kwargs.get(\"temperature\", self.model_config.temperature)\n",
    "        )\n",
    "\n",
    "        # Extract the generated text\n",
    "        return completion.choices[0].message.content\n",
    "    \n",
    "    def chat_completion_stream(self, messages: List[Message], **kwargs) -> Generator[str, None, None]:\n",
    "        \"\"\"Generate a streaming chat completion\"\"\"\n",
    "        # For now, use non-streaming version as a placeholder\n",
    "        # We'll implement proper streaming later\n",
    "        result = self.chat_completion(messages, **kwargs)\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TogetherAiClient():\n",
    "    \"\"\"Client for interacting with models through the TogetherAI API server\n",
    "    We use the openai package\"\"\"\n",
    "    \n",
    "    def __init__(self, model_config: ModelConfig):\n",
    "        \"\"\"Initialize the client with model configuration\"\"\"\n",
    "        self.model_config = model_config\n",
    "\n",
    "        self.client = OpenAI(\n",
    "            base_url=model_config.api_base_url or \"https://api.together.xyz/v1\", # Default to Together AI Inference API if no base URL is provided\n",
    "            api_key=model_config.api_key,\n",
    "        )\n",
    "    \n",
    "    def chat_completion(self, \n",
    "            messages: List[Message], # List of messages conforming to the Message pydantic dataclass\n",
    "            **kwargs\n",
    "            ) -> str:\n",
    "        \"\"\"Generate a chat completion from the Together AI API\"\"\"\n",
    "        # Convert our Message objects to the format expected by the OpenAI client\n",
    "        openai_messages = [{\"role\": msg.role, \"content\": msg.content} for msg in messages]\n",
    "\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=self.model_config.model_name,\n",
    "            messages=openai_messages,\n",
    "            max_tokens=kwargs.get(\"max_tokens\", self.model_config.max_tokens),\n",
    "            temperature=kwargs.get(\"temperature\", self.model_config.temperature),\n",
    "            top_p=kwargs.get(\"top_p\", self.model_config.top_p),\n",
    "            stop=kwargs.get(\"stop\", self.model_config.stop) or [\"<|eot_id|>\",\"<|eom_id|>\"]\n",
    "        )\n",
    "\n",
    "        # Extract the generated text\n",
    "        return completion.choices[0].message.content\n",
    "    \n",
    "    def chat_completion_stream(self,\n",
    "            messages: List[Message], # List of messages conforming to the Message pydantic dataclass\n",
    "            **kwargs) -> Generator[str, None, None]:\n",
    "        \"\"\"Generate a streaming chat completion\"\"\"\n",
    "\n",
    "        stream = self.client.chat.completions.create(\n",
    "            model=self.model_config.model_name,\n",
    "            messages=openai_messages,\n",
    "            max_tokens=kwargs.get(\"max_tokens\", self.model_config.max_tokens),\n",
    "            temperature=kwargs.get(\"temperature\", self.model_config.temperature),\n",
    "            top_p=kwargs.get(\"top_p\", self.model_config.top_p),\n",
    "            stop=kwargs.get(\"stop\", self.model_config.stop) or [\"<|eot_id|>\",\"<|eom_id|>\"],\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        for token in stream:\n",
    "            if hasattr(token, 'choices') and token.choices[0].delta.content is not None:\n",
    "                yield token.choices[0].delta.content\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the LLM client\n",
    "\n",
    "This function creates the client using the available LLM Client classes.\n",
    "It gets the provider from the `model_config`. If it finds a LLM Client Class for this provider, it returns that client. If it doesn't find a LLM Client Class for that provider, it returns a ValueError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_llm_client(model_config: ModelConfig) -> LLMClientProtocol:\n",
    "    \"\"\"\n",
    "    Factory function to create an LLM client based on the provider.\n",
    "    \"\"\"\n",
    "    if model_config.provider.lower() == \"huggingface\":\n",
    "        return HuggingFaceClient(model_config)\n",
    "    if model_config.provider.lower() == \"togetherai\":\n",
    "        return TogetherAiClient(model_config)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported provider: {model_config.provider}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The internal logic of the chat app\n",
    "\n",
    "Now the `BaseChatApp` class is defined. This class is used to instantiate the properties en methods for the internal workings of the chat app. The UI is defined in the `ui` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseChatApp:\n",
    "    \"\"\"Base class for creating configurable chat applications with Gradio\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ChatAppConfig):\n",
    "        \"\"\"Initialize the chat application\"\"\"\n",
    "        self.config = config\n",
    "        self.chat_history = []\n",
    "        self._load_context()\n",
    "        self.client = create_llm_client(config.model)\n",
    "        \n",
    "    def _load_context(self) -> None:\n",
    "        \"\"\"Load context from markdown files\"\"\"\n",
    "        self.context_text = \"\"\n",
    "        for file_path in self.config.context_files:\n",
    "            if file_path.exists() and file_path.is_file():\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    self.context_text += f.read() + \"\\n\\n\"\n",
    "    \n",
    "    def prepare_messages(self, user_message: str) -> List[Message]:\n",
    "        \"\"\"Prepare the messages for the LLM, including system prompt and chat history\"\"\"\n",
    "        messages = []\n",
    "        \n",
    "        # Add system message with prompt and context\n",
    "        system_content = self.config.system_prompt\n",
    "        if self.context_text:\n",
    "            system_content += f\"\\n\\nAdditional information: {self.context_text}\"\n",
    "        \n",
    "        messages.append(Message(role=\"system\", content=system_content))\n",
    "        \n",
    "        # Add chat history\n",
    "        for user_msg, assistant_msg in self.chat_history:\n",
    "            messages.append(Message(role=\"user\", content=user_msg))\n",
    "            messages.append(Message(role=\"assistant\", content=assistant_msg))\n",
    "        \n",
    "        # Add current user message\n",
    "        messages.append(Message(role=\"user\", content=user_message))\n",
    "        \n",
    "        return messages\n",
    "    \n",
    "    def generate_response(self, user_message: str, **kwargs) -> str:\n",
    "        \"\"\"Generate a response to the user message\"\"\"\n",
    "        messages = self.prepare_messages(user_message)\n",
    "        return self.client.chat_completion(messages, **kwargs)\n",
    "    \n",
    "    def generate_stream(self, user_message: str, **kwargs) -> Generator[str, None, None]:\n",
    "        \"\"\"Generate a streaming response to the user message\"\"\"\n",
    "        messages = self.prepare_messages(user_message)\n",
    "        return self.client.chat_completion_stream(messages, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response received: As a philosopher and theologian, I, Aurelius Augustinus, believe that engaging with people around us...\n",
      "\n",
      "Testing with overridden parameters:\n",
      "Response: As a thoughtful individual who seeks wisdom and understanding, engaging with the people around you s...\n"
     ]
    }
   ],
   "source": [
    "# Create a test model config\n",
    "test_config = ModelConfig(\n",
    "    # model_name=\"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.3\", # \"Qwen/QwQ-32B\" is another possibility, but with vision you need another messages format\n",
    "    provider=\"huggingface\",\n",
    "    api_key_env_var=\"HF_API_KEY\",\n",
    "    api_base_url=\"https://router.huggingface.co/hf-inference/v1\",\n",
    "    max_tokens=100,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Create the client\n",
    "client = create_llm_client(test_config)\n",
    "\n",
    "test_messages = [\n",
    "    Message(role=\"system\", content=\"You are Aurelius Augustinus, helping me to think deeply and be humble and thankfull.\"),\n",
    "    Message(role=\"user\", content=\"Why should I engage with the people around me?\")\n",
    "]\n",
    "# Test with a simple prompt\n",
    "try:\n",
    "    response = client.chat_completion(test_messages)\n",
    "    print(f\"Response received: {response[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Test with overriden parameters\n",
    "try:\n",
    "    print(\"\\nTesting with overridden parameters:\")\n",
    "    response = client.chat_completion(test_messages, max_tokens=50, temperature=0.9)\n",
    "    print(f\"Response: {response[:100]}...\")  # Show first 100 chars\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
